# Bloque 7: Preparaci√≥n de Datos para Modelado

## üìã Descripci√≥n General

Este bloque es como **preparar los ingredientes antes de cocinar**. Tenemos los 
datos limpios y las features creadas, pero ahora necesitamos transformarlos al 
formato exacto que los algoritmos de Machine Learning requieren.

---

## üéØ Prop√≥sito y Objetivo

Los objetivos principales de este bloque son:

1. **Separar variables predictoras (X) de la variable objetivo (y)**
2. **Dividir datos en conjuntos de entrenamiento y prueba**
3. **Codificar variables categ√≥ricas** en formato num√©rico
4. **Normalizar variables num√©ricas** a la misma escala
5. **Crear un pipeline de preprocesamiento** automatizado

### ¬øPor qu√© es importante?

**Analog√≠a del examen**: Imagina que vas a tomar un examen:
- **Entrenamiento**: Estudias con ejercicios de pr√°ctica
- **Prueba**: Tomas el examen real con preguntas nuevas

Si estudias con las mismas preguntas del examen, memorizar√°s las respuestas pero 
no aprender√°s realmente. Por eso separamos los datos.

---

## üîë Conceptos Clave y T√©cnicas Utilizadas

### 1. **Separaci√≥n de Variables (X e y)**

```python
X = df_model.drop(['Churn', 'customerID'], axis=1)
y = df_model['Churn'].map({'Yes': 1, 'No': 0})
```

**¬øQu√© hace esto?**

- **X**: Variables predictoras (features) - todo excepto Churn y customerID
- **y**: Variable objetivo - Churn convertido a 1 (Yes) y 0 (No)

**¬øPor qu√© eliminar customerID?**

- Es solo un identificador √∫nico, no tiene poder predictivo
- Ser√≠a como usar el n√∫mero de c√©dula para predecir si alguien se enferma

**Analog√≠a**: X son las pistas que el detective tiene, y es el culpable que debe descubrir.

---

### 2. **Divisi√≥n Train/Test (80/20)**

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)
```

**Par√°metros importantes**:

#### **test_size=0.20**

- 80% de datos para entrenar
- 20% de datos para probar
- **Analog√≠a**: De 100 problemas de matem√°ticas, practicas con 80 y te eval√∫an con 20 nuevos

#### **random_state=42**

- Fija la semilla aleatoria para reproducibilidad
- Siempre obtendremos la misma divisi√≥n
- **Analog√≠a**: Como usar la misma baraja de cartas mezclada de la misma forma cada vez

#### **stratify=y**

- Mantiene la misma proporci√≥n de churn en train y test
- Si hay 27% de churn en total, habr√° ~27% en train y ~27% en test
- **Analog√≠a**: Si una clase tiene 30% ni√±os y 70% ni√±as, al dividir en grupos 
mantienes esa proporci√≥n

**¬øPor qu√© es cr√≠tico?**

- **Sin stratify**: Podr√≠as tener 40% churn en train y 10% en test (desbalance)
- **Con stratify**: Ambos conjuntos son representativos

---

### 3. **Identificaci√≥n de Tipos de Variables**

```python
categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()
numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
```

**¬øPor qu√© separar?**

- Variables categ√≥ricas y num√©ricas requieren transformaciones diferentes
- **Categ√≥ricas**: Necesitan codificaci√≥n (texto ‚Üí n√∫meros)
- **Num√©ricas**: Necesitan normalizaci√≥n (misma escala)

---

### 4. **Codificaci√≥n de Variables Categ√≥ricas (One-Hot Encoding)**

```python
OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore')
```

**¬øQu√© es One-Hot Encoding?**

Convierte categor√≠as en columnas binarias (0 o 1).

**Ejemplo con InternetService**:

- Original: ['DSL', 'Fiber optic', 'No']
- Despu√©s de One-Hot:

  - `InternetService_Fiber optic`: 1 si es Fiber, 0 si no
  - `InternetService_No`: 1 si es No, 0 si no
  - (DSL se infiere cuando ambas son 0)

**Par√°metros**:

- **drop='first'**: Elimina la primera categor√≠a para evitar multicolinealidad
- **sparse=False**: Retorna array denso (m√°s f√°cil de manejar)
- **handle_unknown='ignore'**: Si aparece una categor√≠a nueva en test, la ignora

**Analog√≠a**: Es como tener casillas de verificaci√≥n:

- ‚òê DSL
- ‚òê Fiber optic  
- ‚òê No internet

Marcas la que aplica (1) y dejas las dem√°s vac√≠as (0).

---

### 5. **Normalizaci√≥n de Variables Num√©ricas (StandardScaler)**

```python
StandardScaler()
```

**¬øQu√© hace StandardScaler?**

Transforma los datos para que tengan:

- **Media = 0**
- **Desviaci√≥n est√°ndar = 1**

**F√≥rmula**: `(valor - media) / desviaci√≥n_est√°ndar`

**¬øPor qu√© es necesario?**

**Problema sin normalizaci√≥n**:

- `tenure`: rango 0-72
- `MonthlyCharges`: rango 18-118
- `TotalCharges`: rango 0-8,000+

Algunos algoritmos (como SVM, KNN) son sensibles a la escala. Sin normalizaci√≥n, 
`TotalCharges` dominar√≠a porque tiene valores mucho m√°s grandes.

**Analog√≠a**: Es como convertir todas las medidas a la misma unidad antes de 
compararlas:

- Altura: metros
- Peso: kilogramos
- Edad: a√±os

Sin normalizaci√≥n ser√≠a como comparar metros con mil√≠metros (los mil√≠metros siempre parecer√≠an m√°s importantes por ser n√∫meros m√°s grandes).

---

### 6. **Pipeline de Preprocesamiento**

```python
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(...), categorical_features)
    ]
)
```

**¬øQu√© es un Pipeline?**

Un flujo de trabajo automatizado que aplica transformaciones en orden.

**Ventajas**:

1. **Automatizaci√≥n**: Aplica todas las transformaciones con un solo comando
2. **Consistencia**: Las mismas transformaciones se aplican a train y test
3. **Previene data leakage**: No usa informaci√≥n de test para transformar train
4. **Reproducibilidad**: F√°cil de replicar

**Analog√≠a**: Es como una l√≠nea de ensamblaje en una f√°brica:

- Estaci√≥n 1: Normalizar n√∫meros
- Estaci√≥n 2: Codificar categor√≠as
- Producto final: Datos listos para el modelo

---

### 7. **Aplicaci√≥n del Preprocesamiento**

```python
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```

**Diferencia cr√≠tica**:

- **fit_transform** en train: Aprende los par√°metros (media, desviaci√≥n) Y transforma
- **transform** en test: Solo transforma usando los par√°metros aprendidos de train

**¬øPor qu√© esta diferencia?**

**Analog√≠a del profesor**: 

- El profesor (preprocessor) aprende de los estudiantes de pr√°ctica (train)
- Luego aplica lo aprendido a los estudiantes del examen (test)
- NO debe aprender nada de los estudiantes del examen (evita data leakage)

---

## üìä Resultado de la Preparaci√≥n

**Antes**:

- Variables categ√≥ricas como texto
- Variables num√©ricas en diferentes escalas
- Todo en un solo DataFrame

**Despu√©s**:

- Todo convertido a n√∫meros
- Variables normalizadas (media=0, std=1)
- Listo para alimentar a los modelos

**Dimensiones**:

- **X_train**: ~5,634 filas (80%)
- **X_test**: ~1,409 filas (20%)
- **Columnas**: Aumentan por One-Hot Encoding

---

## üîó Relaci√≥n con el An√°lisis General

Este bloque es el **√∫ltimo paso antes del modelado**:

1. **Cierra el preprocesamiento**: Datos completamente listos
2. **Previene errores comunes**: Data leakage, escalas incorrectas
3. **Optimiza el rendimiento**: Modelos funcionan mejor con datos normalizados
4. **Facilita la evaluaci√≥n**: Divisi√≥n train/test permite medir rendimiento real

---

## üí° Puntos Clave para Recordar

1. **Divisi√≥n 80/20** con stratify para mantener proporciones
2. **One-Hot Encoding** para variables categ√≥ricas
3. **StandardScaler** para variables num√©ricas
4. **Pipeline** automatiza y asegura consistencia
5. **fit_transform** en train, **transform** en test (evita data leakage)
6. **random_state=42** para reproducibilidad

---

## üéì Conclusi√≥n

La preparaci√≥n de datos es como preparar el escenario antes de una obra de teatro: todo debe estar en su lugar, con el formato correcto y listo para la acci√≥n. Un preprocesamiento adecuado es la diferencia entre un modelo que funciona y uno que falla.

**Siguiente paso**: Entrenar m√∫ltiples modelos baseline y comparar su rendimiento.

