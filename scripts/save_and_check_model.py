"""
Script para agregar al final del notebook Telco-Customer-Churn.ipynb
Serializa el modelo y verifica su tamaÃ±o inmediatamente.
"""

import joblib
import json
import os
from datetime import datetime


def save_model_and_check_size(model, preprocessor, X, y, model_metrics):
    """
    Guarda el modelo, preprocessor y metadata, y verifica el tamaÃ±o.
    
    ParÃ¡metros:
    -----------
    model : sklearn model
        Modelo entrenado (ej: best_model)
    preprocessor : sklearn transformer
        Pipeline de preprocesamiento
    X : DataFrame
        Features utilizadas para entrenamiento
    y : Series
        Target variable
    model_metrics : dict
        Diccionario con mÃ©tricas del modelo (roc_auc, recall, precision, f1)
    """
    
    # Crear directorio si no existe
    os.makedirs('models', exist_ok=True)
    
    print("=" * 70)
    print("ðŸ’¾ GUARDANDO MODELO Y VERIFICANDO TAMAÃ‘O")
    print("=" * 70)
    print()
    
    # 1. Guardar modelo
    print("1ï¸âƒ£  Guardando modelo Random Forest...")
    model_path = 'models/churn_model.pkl'
    joblib.dump(model, model_path)
    model_size = os.path.getsize(model_path) / (1024 * 1024)
    print(f"   âœ… Modelo guardado: {model_path}")
    print(f"   ðŸ“Š TamaÃ±o: {model_size:.2f} MB")
    print()
    
    # 2. Guardar preprocessor
    print("2ï¸âƒ£  Guardando preprocessor...")
    preprocessor_path = 'models/preprocessor.pkl'
    joblib.dump(preprocessor, preprocessor_path)
    preprocessor_size = os.path.getsize(preprocessor_path) / (1024 * 1024)
    print(f"   âœ… Preprocessor guardado: {preprocessor_path}")
    print(f"   ðŸ“Š TamaÃ±o: {preprocessor_size:.2f} MB")
    print()
    
    # 3. Guardar metadata
    print("3ï¸âƒ£  Guardando metadata...")
    metadata = {
        'model_type': type(model).__name__,
        'model_params': model.get_params() if hasattr(model, 'get_params') else {},
        'metrics': model_metrics,
        'training_date': datetime.now().isoformat(),
        'features': list(X.columns),
        'n_features': len(X.columns),
        'n_samples_train': len(X),
        'target_name': y.name if hasattr(y, 'name') else 'Churn',
        'model_size_mb': round(model_size, 2),
        'preprocessor_size_mb': round(preprocessor_size, 2)
    }
    
    metadata_path = 'models/metadata.json'
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    metadata_size = os.path.getsize(metadata_path) / (1024 * 1024)
    print(f"   âœ… Metadata guardada: {metadata_path}")
    print(f"   ðŸ“Š TamaÃ±o: {metadata_size:.4f} MB")
    print()
    
    # 4. Resumen de tamaÃ±os
    total_size = model_size + preprocessor_size + metadata_size
    
    print("=" * 70)
    print("ðŸ“¦ RESUMEN DE ARCHIVOS GUARDADOS:")
    print("=" * 70)
    print(f"   Modelo (churn_model.pkl):        {model_size:>10.2f} MB")
    print(f"   Preprocessor (preprocessor.pkl): {preprocessor_size:>10.2f} MB")
    print(f"   Metadata (metadata.json):        {metadata_size:>10.4f} MB")
    print("-" * 70)
    print(f"   TOTAL:                           {total_size:>10.2f} MB")
    print("=" * 70)
    print()
    
    # 5. EvaluaciÃ³n para deployment
    print("ðŸŽ¯ EVALUACIÃ“N PARA DEPLOYMENT:")
    print("-" * 70)
    
    if total_size < 10:
        print("âœ… EXCELENTE: Modelo muy ligero (< 10 MB)")
        print("   âœ… Perfecto para Git (sin Git LFS)")
        print("   âœ… Carga rÃ¡pida en Render/Railway")
        print("   âœ… Bajo consumo de RAM en producciÃ³n")
    elif total_size < 50:
        print("âœ… MUY BUENO: Modelo ligero (< 50 MB)")
        print("   âœ… Puede almacenarse en Git directamente")
        print("   âœ… Deployment rÃ¡pido en Render/Railway")
    elif total_size < 100:
        print("âœ… BUENO: Modelo de tamaÃ±o moderado (< 100 MB)")
        print("   âœ… Puede almacenarse en Git (lÃ­mite 100 MB)")
        print("   âš ï¸  Considera usar Git LFS si crece mÃ¡s")
    else:
        print("âš ï¸  GRANDE: Modelo > 100 MB")
        print("   âŒ NO puede almacenarse directamente en GitHub")
        print("   ðŸ’¡ SOLUCIONES:")
        print("      - Usar Git LFS (Large File Storage)")
        print("      - Almacenar en Google Cloud Storage")
        print("      - Almacenar en AWS S3")
        print("      - Reducir complejidad del modelo")
    
    print()
    
    # 6. EstimaciÃ³n de RAM en producciÃ³n
    estimated_ram = total_size * 3  # Regla general: 3x el tamaÃ±o del modelo
    
    print("ðŸ’¾ ESTIMACIÃ“N DE RAM EN PRODUCCIÃ“N:")
    print("-" * 70)
    print(f"   TamaÃ±o del modelo:               {total_size:>10.2f} MB")
    print(f"   RAM estimada necesaria:          {estimated_ram:>10.2f} MB")
    print()
    
    if estimated_ram < 512:
        print("   âœ… Cabe en Render Free (512 MB RAM)")
        print("   âœ… Cabe en Railway Free (512 MB RAM)")
    elif estimated_ram < 1024:
        print("   âš ï¸  Puede ser justo para Render Free (512 MB RAM)")
        print("   âœ… Cabe en Railway Free (8 GB RAM)")
        print("   ðŸ’¡ Considera Railway si tienes problemas en Render")
    else:
        print("   âŒ Excede Render Free (512 MB RAM)")
        print("   âœ… Cabe en Railway Free (8 GB RAM)")
        print("   ðŸ’¡ RECOMENDACIÃ“N: Usar Railway en lugar de Render")
    
    print()
    print("=" * 70)
    
    # 7. Verificar que se puede cargar
    print("ðŸ” VERIFICANDO QUE EL MODELO SE PUEDE CARGAR...")
    print("-" * 70)
    
    try:
        loaded_model = joblib.load(model_path)
        loaded_preprocessor = joblib.load(preprocessor_path)
        print("   âœ… Modelo cargado correctamente")
        print("   âœ… Preprocessor cargado correctamente")
        
        # Verificar que tiene los mÃ©todos necesarios
        assert hasattr(loaded_model, 'predict'), "Modelo no tiene mÃ©todo predict"
        assert hasattr(loaded_model, 'predict_proba'), "Modelo no tiene mÃ©todo predict_proba"
        print("   âœ… MÃ©todos predict y predict_proba disponibles")
        
    except Exception as e:
        print(f"   âŒ ERROR al cargar el modelo: {e}")
        return False
    
    print()
    print("=" * 70)
    print("âœ… MODELO GUARDADO Y VERIFICADO EXITOSAMENTE")
    print("=" * 70)
    print()
    print("ðŸ“ PRÃ“XIMOS PASOS:")
    print("   1. Ejecutar: python scripts/check_model_size.py")
    print("   2. Revisar el tamaÃ±o total del deployment")
    print("   3. Continuar con la creaciÃ³n de la API Flask")
    print()
    
    return True


# EJEMPLO DE USO EN EL NOTEBOOK:
# ================================
# Agregar esta celda al final del notebook despuÃ©s de entrenar el modelo:

"""
# Guardar modelo y verificar tamaÃ±o
from scripts.save_and_check_model import save_model_and_check_size

model_metrics = {
    'roc_auc': 0.87,
    'recall': 0.83,
    'precision': 0.72,
    'f1_score': 0.77
}

save_model_and_check_size(
    model=best_model,
    preprocessor=preprocessor,
    X=X_train,
    y=y_train,
    model_metrics=model_metrics
)
"""

