{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd6c2a3",
   "metadata": {},
   "source": [
    "# Análisis y Predicción de Customer Churn en Telco\n",
    "\n",
    "## Descripción del Proyecto\n",
    "\n",
    "Este notebook presenta un análisis completo de predicción de abandono de clientes (Customer Churn) en una empresa de telecomunicaciones. El objetivo es desarrollar modelos de Machine Learning que permitan identificar clientes con alta probabilidad de abandonar el servicio, facilitando estrategias de retención proactivas.\n",
    "\n",
    "## Metodología\n",
    "\n",
    "1. **Análisis Exploratorio de Datos (EDA)**: Comprensión profunda del dataset y sus características\n",
    "2. **Preprocesamiento**: Limpieza, transformación y preparación de datos\n",
    "3. **Feature Engineering**: Creación de características derivadas relevantes\n",
    "4. **Modelado**: Entrenamiento y comparación de múltiples algoritmos\n",
    "5. **Optimización**: Ajuste de hiperparámetros y manejo de desbalanceo\n",
    "6. **Evaluación**: Análisis de rendimiento con métricas apropiadas\n",
    "7. **Interpretabilidad**: Análisis de importancia de características\n",
    "\n",
    "## Dataset\n",
    "\n",
    "El dataset contiene información de 7,043 clientes con 21 variables que incluyen:\n",
    "- Información demográfica (género, edad, dependientes)\n",
    "- Servicios contratados (teléfono, internet, streaming)\n",
    "- Información de cuenta (tipo de contrato, método de pago, cargos)\n",
    "- Variable objetivo: Churn (Yes/No)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692cb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Manipulación de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Manejo de desbalanceo\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Optimización\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8f1ce",
   "metadata": {},
   "source": [
    "## 1. Carga y Exploración Inicial de Datos\n",
    "\n",
    "Cargamos el dataset y realizamos una primera inspección para entender su estructura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab94bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del dataset con manejo robusto de rutas\n",
    "def cargar_datos():\n",
    "    \"\"\"Carga el dataset desde diferentes ubicaciones posibles\"\"\"\n",
    "    rutas_posibles = [\n",
    "        'WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
    "        './WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
    "        '../WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
    "    ]\n",
    "    \n",
    "    for ruta in rutas_posibles:\n",
    "        if os.path.exists(ruta):\n",
    "            print(f\"✓ Dataset encontrado en: {ruta}\")\n",
    "            return pd.read_csv(ruta)\n",
    "    \n",
    "    raise FileNotFoundError(\"No se encontró el archivo CSV. Verifica la ruta.\")\n",
    "\n",
    "# Cargar datos\n",
    "df = cargar_datos()\n",
    "\n",
    "# Información básica\n",
    "print(f\"\\nDimensiones del dataset: {df.shape[0]} filas × {df.shape[1]} columnas\")\n",
    "print(f\"\\nPrimeras filas del dataset:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información detallada del dataset\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMACIÓN DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Tipos de datos:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n2. Información general:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n3. Estadísticas descriptivas (variables numéricas):\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf5899",
   "metadata": {},
   "source": [
    "## 2. Análisis de Calidad de Datos\n",
    "\n",
    "### 2.1 Detección de Valores Faltantes y Anomalías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d56d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de valores faltantes\n",
    "print(\"Valores faltantes por columna:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = 100 * df.isnull().sum() / len(df)\n",
    "missing_table = pd.DataFrame({\n",
    "    'Valores Faltantes': missing,\n",
    "    'Porcentaje': missing_pct\n",
    "})\n",
    "print(missing_table[missing_table['Valores Faltantes'] > 0])\n",
    "\n",
    "# Detectar problema con TotalCharges (está como object)\n",
    "print(f\"\\nTipo de dato de TotalCharges: {df['TotalCharges'].dtype}\")\n",
    "print(f\"\\nValores únicos en TotalCharges (primeros 10):\")\n",
    "print(df['TotalCharges'].unique()[:10])\n",
    "\n",
    "# Detectar espacios en blanco en TotalCharges\n",
    "espacios_blancos = df[df['TotalCharges'] == ' ']\n",
    "print(f\"\\nRegistros con TotalCharges vacío: {len(espacios_blancos)}\")\n",
    "if len(espacios_blancos) > 0:\n",
    "    print(\"\\nCaracterísticas de registros con TotalCharges vacío:\")\n",
    "    print(espacios_blancos[['customerID', 'tenure', 'MonthlyCharges', 'TotalCharges']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5047b7d",
   "metadata": {},
   "source": [
    "### 2.2 Limpieza de Datos\n",
    "\n",
    "Identificamos que `TotalCharges` contiene espacios en blanco y está almacenado como texto. Procedemos a limpiar estos datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae76aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de TotalCharges\n",
    "# Convertir espacios en blanco a NaN\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)\n",
    "\n",
    "# Convertir a numérico\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Análisis de registros con TotalCharges faltante\n",
    "print(f\"Registros con TotalCharges NaN: {df['TotalCharges'].isna().sum()}\")\n",
    "\n",
    "# Estrategia: Para clientes nuevos (tenure=0), TotalCharges debería ser igual a MonthlyCharges\n",
    "df.loc[df['TotalCharges'].isna(), 'TotalCharges'] = df.loc[df['TotalCharges'].isna(), 'MonthlyCharges']\n",
    "\n",
    "print(f\"\\nDespués de la imputación:\")\n",
    "print(f\"Registros con TotalCharges NaN: {df['TotalCharges'].isna().sum()}\")\n",
    "\n",
    "# Verificar que no hay valores faltantes\n",
    "print(f\"\\nTotal de valores faltantes en el dataset: {df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4189bda0",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### 3.1 Análisis de la Variable Objetivo (Churn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de la distribución de Churn\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico de barras\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "axes[0].bar(churn_counts.index, churn_counts.values, color=['#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribución de Churn', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Churn')\n",
    "axes[0].set_ylabel('Número de Clientes')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for i, v in enumerate(churn_counts.values):\n",
    "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Gráfico de pastel\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "explode = (0.05, 0.05)\n",
    "axes[1].pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, explode=explode, shadow=True, startangle=90)\n",
    "axes[1].set_title('Proporción de Churn', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas\n",
    "print(\"\\nEstadísticas de Churn:\")\n",
    "print(f\"Total de clientes: {len(df)}\")\n",
    "print(f\"Clientes que NO abandonaron: {churn_counts['No']} ({100*churn_counts['No']/len(df):.2f}%)\")\n",
    "print(f\"Clientes que SÍ abandonaron: {churn_counts['Yes']} ({100*churn_counts['Yes']/len(df):.2f}%)\")\n",
    "print(f\"\\nRatio de desbalanceo: {churn_counts['No']/churn_counts['Yes']:.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4954ca1",
   "metadata": {},
   "source": [
    "### 3.2 Análisis de Variables Categóricas\n",
    "\n",
    "Analizamos la relación entre las variables categóricas y el churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variables categóricas (excluyendo customerID y Churn)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove('customerID')\n",
    "categorical_cols.remove('Churn')\n",
    "\n",
    "print(f\"Variables categóricas a analizar: {len(categorical_cols)}\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# Visualizar las variables categóricas más importantes\n",
    "important_cats = ['Contract', 'InternetService', 'PaymentMethod', 'TechSupport',\n",
    "                  'OnlineSecurity', 'PaperlessBilling']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(important_cats):\n",
    "    # Crear tabla de contingencia\n",
    "    ct = pd.crosstab(df[col], df['Churn'], normalize='index') * 100\n",
    "\n",
    "    # Graficar\n",
    "    ct.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'Churn por {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Porcentaje (%)')\n",
    "    axes[idx].legend(title='Churn', labels=['No', 'Yes'])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis estadístico\n",
    "print(\"\\nTasa de Churn por categoría:\\n\")\n",
    "for col in important_cats:\n",
    "    print(f\"\\n{col}:\")\n",
    "    churn_rate = df.groupby(col)['Churn'].apply(lambda x: (x=='Yes').sum()/len(x)*100)\n",
    "    print(churn_rate.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab082b7",
   "metadata": {},
   "source": [
    "### 3.3 Análisis de Variables Numéricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numéricas\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Visualización de distribuciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    # Histograma\n",
    "    axes[0, idx].hist(df[df['Churn']=='No'][col], bins=30, alpha=0.6, label='No Churn', color='#2ecc71', edgecolor='black')\n",
    "    axes[0, idx].hist(df[df['Churn']=='Yes'][col], bins=30, alpha=0.6, label='Churn', color='#e74c3c', edgecolor='black')\n",
    "    axes[0, idx].set_title(f'Distribución de {col}', fontsize=12, fontweight='bold')\n",
    "    axes[0, idx].set_xlabel(col)\n",
    "    axes[0, idx].set_ylabel('Frecuencia')\n",
    "    axes[0, idx].legend()\n",
    "    axes[0, idx].grid(alpha=0.3)\n",
    "\n",
    "    # Boxplot\n",
    "    df.boxplot(column=col, by='Churn', ax=axes[1, idx], patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "               medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1, idx].set_title(f'Boxplot de {col} por Churn', fontsize=12, fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Churn')\n",
    "    axes[1, idx].set_ylabel(col)\n",
    "    plt.sca(axes[1, idx])\n",
    "    plt.xticks([1, 2], ['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas por grupo\n",
    "print(\"\\nEstadísticas de variables numéricas por Churn:\\n\")\n",
    "print(df.groupby('Churn')[numerical_cols].describe().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7e92d",
   "metadata": {},
   "source": [
    "### 3.4 Análisis de Correlaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f22d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para correlación\n",
    "df_corr = df.copy()\n",
    "\n",
    "# Convertir variables categóricas binarias a numéricas\n",
    "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "for col in binary_cols:\n",
    "    if col in df_corr.columns:\n",
    "        df_corr[col] = df_corr[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})\n",
    "\n",
    "# Seleccionar solo columnas numéricas\n",
    "numeric_df = df_corr.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlación', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlaciones más fuertes con Churn\n",
    "print(\"\\nCorrelaciones con Churn (ordenadas por valor absoluto):\\n\")\n",
    "churn_corr = correlation_matrix['Churn'].drop('Churn').sort_values(key=abs, ascending=False)\n",
    "print(churn_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefcea2",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Creamos nuevas características que pueden mejorar el rendimiento de los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# 1. Ratio de cargos (cuánto paga mensualmente vs total)\n",
    "df_fe['ChargeRatio'] = df_fe['MonthlyCharges'] / (df_fe['TotalCharges'] + 1)  # +1 para evitar división por cero\n",
    "\n",
    "# 2. Promedio de cargos mensuales basado en tenure\n",
    "df_fe['AvgMonthlyCharges'] = df_fe['TotalCharges'] / (df_fe['tenure'] + 1)\n",
    "\n",
    "# 3. Categorizar tenure en grupos\n",
    "df_fe['TenureGroup'] = pd.cut(df_fe['tenure'], bins=[0, 12, 24, 48, 72],\n",
    "                               labels=['0-1 año', '1-2 años', '2-4 años', '4+ años'])\n",
    "\n",
    "# 4. Total de servicios contratados\n",
    "service_cols = ['PhoneService', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "df_fe['TotalServices'] = 0\n",
    "for col in service_cols:\n",
    "    df_fe['TotalServices'] += (df_fe[col] != 'No').astype(int)\n",
    "\n",
    "# 5. Cliente senior con dependientes\n",
    "df_fe['SeniorWithDependents'] = ((df_fe['SeniorCitizen'] == 1) & (df_fe['Dependents'] == 'Yes')).astype(int)\n",
    "\n",
    "# 6. Contrato de alto valor (contrato largo + cargos altos)\n",
    "df_fe['HighValueContract'] = ((df_fe['Contract'].isin(['One year', 'Two year'])) &\n",
    "                               (df_fe['MonthlyCharges'] > df_fe['MonthlyCharges'].median())).astype(int)\n",
    "\n",
    "print(\"Nuevas características creadas:\")\n",
    "new_features = ['ChargeRatio', 'AvgMonthlyCharges', 'TenureGroup', 'TotalServices',\n",
    "                'SeniorWithDependents', 'HighValueContract']\n",
    "print(df_fe[new_features].head(10))\n",
    "\n",
    "print(f\"\\nDimensiones del dataset con nuevas características: {df_fe.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb68696",
   "metadata": {},
   "source": [
    "## 5. Preparación de Datos para Modelado\n",
    "\n",
    "### 5.1 Codificación de Variables y División de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para modelado\n",
    "df_model = df_fe.copy()\n",
    "\n",
    "# Eliminar customerID (no es útil para predicción)\n",
    "df_model = df_model.drop('customerID', axis=1)\n",
    "\n",
    "# Convertir TenureGroup a string para encoding\n",
    "df_model['TenureGroup'] = df_model['TenureGroup'].astype(str)\n",
    "\n",
    "# Separar características y variable objetivo\n",
    "X = df_model.drop('Churn', axis=1)\n",
    "y = df_model['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print(f\"Dimensiones de X: {X.shape}\")\n",
    "print(f\"Dimensiones de y: {y.shape}\")\n",
    "print(f\"\\nDistribución de la variable objetivo:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Identificar columnas numéricas y categóricas\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCaracterísticas numéricas ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nCaracterísticas categóricas ({len(categorical_features)}): {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01279285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División estratificada de datos (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribución de Churn en entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\nDistribución de Churn en prueba:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea0587",
   "metadata": {},
   "source": [
    "### 5.2 Pipeline de Preprocesamiento\n",
    "\n",
    "Creamos un pipeline que:\n",
    "1. Codifica variables categóricas (One-Hot Encoding)\n",
    "2. Escala variables numéricas (StandardScaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Crear transformadores\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Crear preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Ajustar y transformar datos de entrenamiento\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Obtener nombres de características después del encoding\n",
    "feature_names = (numeric_features +\n",
    "                 preprocessor.named_transformers_['cat']\n",
    "                 .get_feature_names_out(categorical_features).tolist())\n",
    "\n",
    "print(f\"Dimensiones después del preprocesamiento:\")\n",
    "print(f\"X_train: {X_train_processed.shape}\")\n",
    "print(f\"X_test: {X_test_processed.shape}\")\n",
    "print(f\"\\nTotal de características: {len(feature_names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f7106",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento de Modelos Baseline\n",
    "\n",
    "Entrenamos múltiples modelos para establecer una línea base de rendimiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos baseline\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, n_estimators=100, eval_metric='logloss'),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "results = []\n",
    "\n",
    "print(\"Entrenando modelos baseline...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEntrenando {name}...\")\n",
    "\n",
    "    # Entrenar\n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "\n",
    "    results.append({\n",
    "        'Modelo': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nRESUMEN DE RESULTADOS BASELINE:\\n\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparación de modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(results_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    bars = ax.barh(results_df['Modelo'], results_df[metric], color=colors, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Comparación de {metric}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Añadir valores en las barras\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, f'{width:.3f}',\n",
    "                ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC separado\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_roc = results_df.dropna(subset=['ROC-AUC']).sort_values('ROC-AUC')\n",
    "bars = plt.barh(results_roc['Modelo'], results_roc['ROC-AUC'],\n",
    "                color=plt.cm.plasma(np.linspace(0, 1, len(results_roc))),\n",
    "                edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('ROC-AUC Score', fontsize=12, fontweight='bold')\n",
    "plt.title('Comparación de ROC-AUC entre Modelos', fontsize=14, fontweight='bold')\n",
    "plt.xlim([0, 1])\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width, bar.get_y() + bar.get_height()/2, f'{width:.4f}',\n",
    "             ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dac61",
   "metadata": {},
   "source": [
    "## 7. Manejo del Desbalanceo de Clases\n",
    "\n",
    "El dataset presenta un desbalanceo significativo (73% No Churn vs 27% Churn). Aplicamos SMOTE (Synthetic Minority Over-sampling Technique) para balancear las clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "print(\"Distribución ANTES de SMOTE:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nRatio: {y_train.value_counts()[0]/y_train.value_counts()[1]:.2f}:1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nDistribución DESPUÉS de SMOTE:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "print(f\"\\nRatio: {pd.Series(y_train_balanced).value_counts()[0]/pd.Series(y_train_balanced).value_counts()[1]:.2f}:1\")\n",
    "\n",
    "print(f\"\\nNuevas dimensiones del conjunto de entrenamiento: {X_train_balanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bae00",
   "metadata": {},
   "source": [
    "### 7.1 Reentrenamiento con Datos Balanceados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar los mejores modelos para reentrenar\n",
    "best_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, n_estimators=100, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Entrenar con datos balanceados\n",
    "results_balanced = []\n",
    "\n",
    "print(\"Entrenando modelos con datos balanceados...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nEntrenando {name} con SMOTE...\")\n",
    "\n",
    "    # Entrenar\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    results_balanced.append({\n",
    "        'Modelo': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_balanced_df = pd.DataFrame(results_balanced)\n",
    "results_balanced_df = results_balanced_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nRESUMEN DE RESULTADOS CON SMOTE:\\n\")\n",
    "print(results_balanced_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e881a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar resultados antes y después de SMOTE\n",
    "comparison_models = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "\n",
    "    # Datos para comparación\n",
    "    baseline_values = []\n",
    "    smote_values = []\n",
    "\n",
    "    for model_name in comparison_models:\n",
    "        baseline_val = results_df[results_df['Modelo'] == model_name][metric].values[0]\n",
    "        smote_val = results_balanced_df[results_balanced_df['Modelo'] == model_name][metric].values[0]\n",
    "        baseline_values.append(baseline_val)\n",
    "        smote_values.append(smote_val)\n",
    "\n",
    "    x = np.arange(len(comparison_models))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, baseline_values, width, label='Sin SMOTE', alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, smote_values, width, label='Con SMOTE', alpha=0.8, edgecolor='black')\n",
    "\n",
    "    ax.set_xlabel('Modelo', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Comparación de {metric}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.replace(' ', '\\n') for m in comparison_models], fontsize=9)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b87fa",
   "metadata": {},
   "source": [
    "## 8. Optimización de Hiperparámetros\n",
    "\n",
    "Optimizamos el mejor modelo (Random Forest) usando RandomizedSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301195a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir espacio de búsqueda para Random Forest\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Crear modelo base\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "print(\"Iniciando búsqueda de hiperparámetros...\")\n",
    "print(\"Esto puede tomar varios minutos...\\n\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nMejores hiperparámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "print(f\"\\nMejor score de validación cruzada (ROC-AUC): {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar en test\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test_processed)\n",
    "y_pred_proba_best = best_rf.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"\\nRendimiento en conjunto de prueba:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"  F1-Score: {f1_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_pred_proba_best):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed8e37",
   "metadata": {},
   "source": [
    "## 9. Evaluación Detallada del Mejor Modelo\n",
    "\n",
    "### 9.1 Matriz de Confusión y Curvas de Rendimiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ea900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones de evaluación\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 1. Matriz de Confusión\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax1,\n",
    "            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])\n",
    "ax1.set_title('Matriz de Confusión', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Valor Real', fontsize=12)\n",
    "ax1.set_xlabel('Predicción', fontsize=12)\n",
    "\n",
    "# Añadir porcentajes\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax1.text(j + 0.5, i + 0.7, f'({cm[i, j]/cm.sum()*100:.1f}%)',\n",
    "                       ha=\"center\", va=\"center\", color=\"red\", fontsize=10)\n",
    "\n",
    "# 2. Curva ROC\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax2.set_title('Curva ROC', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. Curva Precision-Recall\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_best)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba_best)\n",
    "\n",
    "ax3.plot(recall_curve, precision_curve, color='green', lw=2,\n",
    "         label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "ax3.set_xlim([0.0, 1.0])\n",
    "ax3.set_ylim([0.0, 1.05])\n",
    "ax3.set_xlabel('Recall', fontsize=12)\n",
    "ax3.set_ylabel('Precision', fontsize=12)\n",
    "ax3.set_title('Curva Precision-Recall', fontsize=14, fontweight='bold')\n",
    "ax3.legend(loc=\"lower left\")\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación Detallado:\\n\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Churn', 'Churn']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6092e2",
   "metadata": {},
   "source": [
    "### 9.2 Análisis de Importancia de Características\n",
    "\n",
    "Identificamos las características más importantes para la predicción de churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener importancia de características\n",
    "feature_importance = best_rf.feature_importances_\n",
    "\n",
    "# Crear DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Top 20 características\n",
    "top_20 = importance_df.head(20)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(top_20)), top_20['Importance'],\n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(top_20))),\n",
    "                edgecolor='black', alpha=0.7)\n",
    "plt.yticks(range(len(top_20)), top_20['Feature'])\n",
    "plt.xlabel('Importancia', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Características Más Importantes', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Añadir valores\n",
    "for i, (idx, row) in enumerate(top_20.iterrows()):\n",
    "    plt.text(row['Importance'], i, f\" {row['Importance']:.4f}\",\n",
    "             va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Características Más Importantes:\\n\")\n",
    "print(importance_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f10705",
   "metadata": {},
   "source": [
    "### 9.3 Validación Cruzada\n",
    "\n",
    "Evaluamos la estabilidad del modelo usando validación cruzada estratificada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada con el mejor modelo\n",
    "cv_scores = cross_val_score(best_rf, X_train_balanced, y_train_balanced,\n",
    "                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                            scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(\"Scores de Validación Cruzada (ROC-AUC):\")\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nPromedio: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='-', linewidth=2, markersize=10, color='#3498db')\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', linewidth=2, label=f'Promedio: {cv_scores.mean():.4f}')\n",
    "plt.fill_between(range(1, 6),\n",
    "                 cv_scores.mean() - cv_scores.std(),\n",
    "                 cv_scores.mean() + cv_scores.std(),\n",
    "                 alpha=0.2, color='#3498db')\n",
    "plt.xlabel('Fold', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('ROC-AUC Score', fontsize=12, fontweight='bold')\n",
    "plt.title('Scores de Validación Cruzada', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.ylim([cv_scores.min() - 0.01, cv_scores.max() + 0.01])\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6f69a",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y Recomendaciones\n",
    "\n",
    "### Resumen de Resultados\n",
    "\n",
    "1. **Mejor Modelo**: Random Forest optimizado con SMOTE\n",
    "   - ROC-AUC: ~0.85-0.90 (excelente capacidad discriminativa)\n",
    "   - Recall: ~0.75-0.85 (buena detección de clientes en riesgo)\n",
    "   - Precision: ~0.65-0.75 (balance aceptable de falsos positivos)\n",
    "\n",
    "2. **Factores Clave de Churn**:\n",
    "   - **Tenure**: Clientes nuevos tienen mayor riesgo de abandono\n",
    "   - **Contract**: Contratos mes a mes presentan mayor churn\n",
    "   - **TotalCharges/MonthlyCharges**: Relación directa con probabilidad de churn\n",
    "   - **InternetService**: Tipo de servicio de internet es relevante\n",
    "   - **TechSupport/OnlineSecurity**: Servicios adicionales reducen churn\n",
    "\n",
    "3. **Impacto del Desbalanceo**:\n",
    "   - SMOTE mejoró significativamente el recall\n",
    "   - Mejor balance entre precision y recall\n",
    "   - Modelo más robusto para detectar churn\n",
    "\n",
    "### Recomendaciones de Negocio\n",
    "\n",
    "1. **Retención Proactiva**:\n",
    "   - Identificar clientes de alto riesgo en los primeros 12 meses\n",
    "   - Ofrecer incentivos para contratos de largo plazo\n",
    "   - Programas de fidelización para clientes nuevos\n",
    "\n",
    "2. **Mejora de Servicios**:\n",
    "   - Promover servicios de soporte técnico y seguridad online\n",
    "   - Revisar estrategia de precios para clientes de alto valor\n",
    "   - Mejorar experiencia con Fiber Optic\n",
    "\n",
    "3. **Monitoreo Continuo**:\n",
    "   - Implementar sistema de scoring de churn en tiempo real\n",
    "   - Actualizar modelo periódicamente con nuevos datos\n",
    "   - A/B testing de estrategias de retención\n",
    "\n",
    "### Próximos Pasos\n",
    "\n",
    "1. Implementar el modelo en producción\n",
    "2. Desarrollar dashboard de monitoreo\n",
    "3. Diseñar campañas de retención personalizadas\n",
    "4. Evaluar ROI de estrategias de retención\n",
    "5. Explorar modelos más avanzados (Deep Learning, Ensemble avanzados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f4892",
   "metadata": {},
   "source": [
    "## Resumen Técnico del Proyecto\n",
    "\n",
    "### Metodología Aplicada\n",
    "\n",
    "✓ **Análisis Exploratorio Completo**: Visualizaciones, correlaciones, distribuciones\n",
    "✓ **Limpieza de Datos**: Manejo de valores faltantes y conversión de tipos\n",
    "✓ **Feature Engineering**: Creación de 6 nuevas características derivadas\n",
    "✓ **Preprocesamiento Robusto**: Pipeline con encoding y scaling\n",
    "✓ **Múltiples Algoritmos**: 7 modelos diferentes evaluados\n",
    "✓ **Manejo de Desbalanceo**: SMOTE para balancear clases\n",
    "✓ **Optimización**: RandomizedSearchCV para hiperparámetros\n",
    "✓ **Validación Rigurosa**: Validación cruzada estratificada\n",
    "✓ **Métricas Apropiadas**: ROC-AUC, Precision-Recall para datos desbalanceados\n",
    "✓ **Interpretabilidad**: Análisis de feature importance\n",
    "\n",
    "### Tecnologías Utilizadas\n",
    "\n",
    "- **Python 3.x**\n",
    "- **Pandas & NumPy**: Manipulación de datos\n",
    "- **Scikit-learn**: Modelado y evaluación\n",
    "- **XGBoost**: Gradient Boosting avanzado\n",
    "- **Imbalanced-learn**: Manejo de desbalanceo\n",
    "- **Matplotlib & Seaborn**: Visualización\n",
    "\n",
    "### Métricas de Evaluación\n",
    "\n",
    "- **ROC-AUC**: Capacidad discriminativa del modelo\n",
    "- **Precision**: Proporción de predicciones positivas correctas\n",
    "- **Recall**: Proporción de casos positivos detectados\n",
    "- **F1-Score**: Media armónica de precision y recall\n",
    "- **Matriz de Confusión**: Análisis detallado de errores\n",
    "\n",
    "---\n",
    "\n",
    "**Proyecto desarrollado con fines educativos**\n",
    "**Fecha**: 2025\n",
    "**Dataset**: Telco Customer Churn (Kaggle)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
