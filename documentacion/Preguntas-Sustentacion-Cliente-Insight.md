---
title: "Preguntas de Sustentaci√≥n - Proyecto de Predicci√≥n de Customer Churn"
subtitle: "An√°lisis de Customer Churn"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 3
  html_document:
    toc: true
header-includes:
  - \usepackage{fontspec}
  - \usepackage{unicode-math}
---
# üìã Preguntas de Sustentaci√≥n - Proyecto de Predicci√≥n de Customer Churn

## Informaci√≥n del Proyecto
- **Dataset**: Telco Customer Churn (7,043 clientes)
- **Objetivo**: Predecir qu√© clientes abandonar√°n el servicio de telecomunicaciones
- **Mejor Modelo**: Logistic Regression Optimizado (ROC-AUC: 0.8503)
- **T√©cnica de Balanceo**: Undersampling (seleccionada autom√°ticamente por mejor ROC-AUC)

---

# üìä CATEGOR√çA 1: EXPLORACI√ìN Y COMPRENSI√ìN DE DATOS

## Pregunta 1
**¬øQu√© es el "churn" y por qu√© es importante predecirlo en una empresa de telecomunicaciones?**

**Respuesta**: El churn (o tasa de abandono) representa el porcentaje de clientes que dejan de usar los servicios de una empresa en un per√≠odo determinado. En el dataset analizado, el 26.5% de los clientes abandonaron el servicio. Predecirlo es crucial porque adquirir un nuevo cliente cuesta entre 5 y 25 veces m√°s que retener uno existente. Con un modelo predictivo, la empresa puede identificar clientes en riesgo y aplicar estrategias de retenci√≥n proactivas.

**Analog√≠a**: Es como un m√©dico que detecta s√≠ntomas tempranos de una enfermedad. Si identificas a tiempo que un paciente (cliente) est√° "enfermo" (insatisfecho), puedes aplicar un tratamiento (oferta de retenci√≥n) antes de que sea demasiado tarde.

**Mini-glosario**:

- **Churn Rate**: Porcentaje de clientes que abandonan en un per√≠odo
- **Customer Lifetime Value (LTV)**: Valor total que un cliente genera durante su relaci√≥n con la empresa
- **Retenci√≥n**: Estrategias para mantener a los clientes activos

---

## Pregunta 2
**¬øCu√°l es la distribuci√≥n de la variable objetivo (Churn) en el dataset y qu√© problema representa?**

**Respuesta**: El dataset presenta un desbalance de clases: 73% de clientes No Churn vs 27% Churn (ratio 2.7:1). Este desbalance es problem√°tico porque los algoritmos de ML tienden a favorecer la clase mayoritaria, prediciendo casi siempre "No Churn" y fallando en detectar los casos que realmente importan (los clientes que s√≠ abandonar√°n).

**Analog√≠a**: Imagina un detector de incendios que funciona el 99% del tiempo, pero falla justo cuando hay fuego real. Un modelo que predice siempre "No Churn" tendr√≠a 73% de accuracy, pero ser√≠a in√∫til para el negocio.

**Mini-glosario**:

- **Desbalance de clases**: Cuando una categor√≠a tiene muchos m√°s ejemplos que otra
- **Clase mayoritaria/minoritaria**: La categor√≠a con m√°s/menos ejemplos
- **Accuracy paradox**: Alta precisi√≥n general pero fallo en la clase importante

---

## Pregunta 3
**¬øQu√© variables del dataset son num√©ricas y cu√°les son categ√≥ricas? ¬øPor qu√© es importante distinguirlas?**

**Respuesta**: 

- **Num√©ricas (3)**: tenure (meses de antig√ºedad),\
MonthlyCharges (cargo mensual),\
TotalCharges (cargo total)
- **Categ√≥ricas (17)**: gender (g√©nero),\
SeniorCitizen (ciudadano mayor/tercera edad),\
Partner (pareja),\
Dependents (dependientes),\
PhoneService (servicio telef√≥nico),\
MultipleLines (l√≠neas m√∫ltiples),\
InternetService (servicio de internet),\
OnlineSecurity (seguridad en l√≠nea),\
OnlineBackup (respaldo en l√≠nea),\
DeviceProtection (protecci√≥n de dispositivos),\
TechSupport (soporte t√©cnico),\
StreamingTV (streaming de TV),\
StreamingMovies (streaming de pel√≠culas),\
Contract (tipo de contrato),\
PaperlessBilling (facturaci√≥n sin papel),\
PaymentMethod (m√©todo de pago)

Es importante distinguirlas porque requieren diferentes t√©cnicas de preprocesamiento: las num√©ricas necesitan escalado (StandardScaler) y las categ√≥ricas necesitan codificaci√≥n (OneHotEncoder).

**Analog√≠a**: Es como preparar ingredientes para cocinar: las verduras (categ√≥ricas) se cortan de una forma y las carnes (num√©ricas) de otra. No puedes aplicar el mismo proceso a todos.

**Mini-glosario**:

- **Variable num√©rica**: Datos que representan cantidades medibles
- **Variable categ√≥rica**: Datos que representan grupos o categor√≠as
- **Preprocesamiento**: Transformaci√≥n de datos antes del modelado

---

## Pregunta 4
**¬øQu√© patrones encontraste en el an√°lisis exploratorio respecto a los clientes que abandonan?**

**Respuesta**: Los principales patrones identificados fueron:

1. **Tenure bajo**: Clientes con menos de 12 meses tienen mayor probabilidad de churn
2. **Contratos mes a mes**: 42% de churn vs 3% en contratos de 2 a√±os
3. **Sin servicios adicionales**: Clientes sin OnlineSecurity (seguridad en l√≠nea), TechSupport (soporte t√©cnico) tienen m√°s churn
4. **Cargos mensuales altos**: Correlaci√≥n positiva con el abandono
5. **Fiber optic**: Mayor churn que DSL o sin internet

**Analog√≠a**: Es como analizar por qu√© los estudiantes abandonan una universidad: los de primer a√±o (tenure bajo), sin beca (sin servicios adicionales) y con matr√≠culas altas (cargos altos) tienen m√°s probabilidad de desertar.

**Mini-glosario**:

- **EDA (Exploratory Data Analysis)**: An√°lisis inicial para entender los datos
- **Correlaci√≥n**: Relaci√≥n estad√≠stica entre dos variables
- **Patr√≥n**: Tendencia recurrente en los datos

---

## Pregunta 5
**¬øC√≥mo manejaste los valores faltantes en el dataset?**

**Respuesta**: El dataset ten√≠a 11 valores faltantes en la columna TotalCharges, correspondientes a clientes nuevos (tenure=0) donde el cargo total era un espacio en blanco. Se convirti√≥ la columna a num√©rico con `pd.to_numeric(errors='coerce')` y se imputaron los valores nulos con el valor de MonthlyCharges, ya que un cliente nuevo en su primer mes tendr√≠a un cargo total igual a su cargo mensual.

**Analog√≠a**: Es como completar el "total pagado" de un cliente nuevo usando su primera factura mensual. Si su cargo mensual es $50, su cargo total inicial tambi√©n ser√° $50.

**Mini-glosario**:

- **Valores faltantes (NaN)**: Datos ausentes en el dataset
- **Imputaci√≥n**: T√©cnica para rellenar valores faltantes
- **Coerci√≥n**: Forzar la conversi√≥n de un tipo de dato a otro

---

# üîß CATEGOR√çA 2: PREPROCESAMIENTO Y FEATURE ENGINEERING

## Pregunta 6
**¬øQu√© es Feature Engineering y qu√© nuevas caracter√≠sticas creaste?**

**Respuesta**: Feature Engineering es el proceso de crear nuevas variables a partir de las existentes para mejorar el poder predictivo del modelo. Se crearon 2 caracter√≠sticas:

1. **Charge_Ratio**: MonthlyCharges / TotalCharges (ratio de cargos)
2. **Total_Services**: Suma de todos los servicios contratados

Estas caracter√≠sticas capturan informaci√≥n que no estaba expl√≠cita en las variables originales.

**Analog√≠a**: Es como un chef que combina ingredientes b√°sicos para crear una salsa especial. Los ingredientes individuales son buenos, pero la combinaci√≥n aporta un sabor √∫nico que mejora el plato.

**Mini-glosario**:

- **Feature**: Variable o caracter√≠stica usada para entrenar el modelo
- **Feature Engineering**: Creaci√≥n de nuevas variables derivadas
- **Poder predictivo**: Capacidad de una variable para predecir el resultado

---

## Pregunta 7
**¬øPor qu√© es necesario escalar las variables num√©ricas?**

**Respuesta**: El escalado es necesario porque las variables num√©ricas tienen diferentes rangos: tenure va de 0-72 meses, MonthlyCharges de 18-118 d√≥lares, y TotalCharges de 0-8,684 d√≥lares. Sin escalado, los algoritmos como SVM, KNN y redes neuronales dar√≠an m√°s peso a las variables con valores m√°s grandes. StandardScaler transforma cada variable para tener media 0 y desviaci√≥n est√°ndar 1.

**Analog√≠a**: Es como convertir diferentes monedas a una sola para poder compararlas. No puedes sumar directamente 100 yenes con 50 euros; necesitas convertirlos a la misma escala.

**Mini-glosario**:

- **StandardScaler**: T√©cnica que normaliza datos a media 0 y std 1
- **Normalizaci√≥n**: Ajustar valores a un rango com√∫n
- **Rango**: Diferencia entre el valor m√°ximo y m√≠nimo

---

## Pregunta 8
**¬øQu√© es OneHotEncoder y por qu√© se usa para variables categ√≥ricas?**

**Respuesta**: OneHotEncoder convierte variables categ√≥ricas en columnas binarias (0/1). Por ejemplo, la variable "Contract" con valores {Month-to-month, One year, Two year} se convierte en 3 columnas: Contract_Month-to-month, Contract_One year, Contract_Two year. Esto es necesario porque los algoritmos de ML trabajan con n√∫meros, no con texto.

**Analog√≠a**: Es como traducir un men√∫ de restaurante a pictogramas para que personas de cualquier idioma puedan entenderlo. Cada plato tiene su propio s√≠mbolo √∫nico.

**Mini-glosario**:

- **OneHotEncoder**: T√©cnica de codificaci√≥n binaria para categor√≠as
- **Variable dummy**: Columna binaria creada por OneHotEncoding
- **Codificaci√≥n**: Transformaci√≥n de texto a n√∫meros

---

## Pregunta 9
**¬øQu√© es un Pipeline en scikit-learn y por qu√© lo usaste?**

**Respuesta**: Un Pipeline es una secuencia de transformaciones que se aplican autom√°ticamente en orden. En el proyecto se us√≥ ColumnTransformer con dos pipelines: uno para variables num√©ricas (StandardScaler) y otro para categ√≥ricas (OneHotEncoder). Esto garantiza que las mismas transformaciones se apliquen consistentemente en entrenamiento y predicci√≥n, evitando data leakage.

**Analog√≠a**: Es como una l√≠nea de ensamblaje en una f√°brica: cada estaci√≥n hace una tarea espec√≠fica en orden, y el producto final siempre pasa por el mismo proceso.

**Mini-glosario**:

- **Pipeline**: Cadena de transformaciones secuenciales
- **ColumnTransformer**: Aplica diferentes transformaciones a diferentes columnas
- **Data leakage**: Filtraci√≥n de informaci√≥n del test al entrenamiento

---

## Pregunta 10
**¬øC√≥mo dividiste los datos en entrenamiento y prueba?**

**Respuesta**: Se us√≥ `train_test_split` con 80% para entrenamiento y 20% para prueba, con `stratify=y` para mantener la proporci√≥n de clases en ambos conjuntos, y `random_state=42` para reproducibilidad. Esto result√≥ en 5,634 muestras de entrenamiento y 1,409 de prueba.

**Analog√≠a**: Es como dividir un mazo de cartas para un juego: necesitas que ambos jugadores tengan una proporci√≥n similar de cartas rojas y negras para que el juego sea justo.

**Mini-glosario**:

- **Train/Test split**: Divisi√≥n de datos para entrenamiento y evaluaci√≥n
- **Stratify**: Mantener proporciones de clases en la divisi√≥n
- **Random state**: Semilla para reproducibilidad

---

# ‚öñÔ∏è CATEGOR√çA 3: MANEJO DE DESBALANCE DE CLASES

## Pregunta 11
**¬øQu√© t√©cnicas de balanceo de clases evaluaste y cu√°l fue la mejor?**

**Respuesta**: Se evaluaron 3 t√©cnicas:

1. **Undersampling** ‚≠ê: Reduce la clase mayoritaria (ROC-AUC: 0.8277, Recall: 77.01%)
2. **SMOTE + Tomek Links**: SMOTE + eliminaci√≥n de ejemplos ambiguos (ROC-AUC: 0.8273)
3. **SMOTE**: Crea ejemplos sint√©ticos de la clase minoritaria (ROC-AUC: 0.8256)

**Undersampling fue seleccionado** por obtener el mejor ROC-AUC y el mejor Recall, siendo adem√°s la t√©cnica m√°s r√°pida (0.58s vs 1.78s de SMOTE+Tomek).

**Analog√≠a**: Es como equilibrar un aula donde hay 73 estudiantes de un grupo y 27 de otro. Puedes: (1) clonar estudiantes del grupo peque√±o (SMOTE), (2) clonar y remover los m√°s confusos (SMOTE+Tomek), o (3) sacar estudiantes del grupo grande (Undersampling).

**Mini-glosario**:

- **SMOTE**: Synthetic Minority Over-sampling Technique
- **Oversampling**: Aumentar ejemplos de la clase minoritaria
- **Undersampling**: Reducir ejemplos de la clase mayoritaria

---

## Pregunta 12
**¬øC√≥mo funciona SMOTE a nivel t√©cnico?**

**Respuesta**: SMOTE (Synthetic Minority Over-sampling Technique) genera ejemplos sint√©ticos de la clase minoritaria mediante interpolaci√≥n. Para cada ejemplo de la clase minoritaria: (1) encuentra sus k vecinos m√°s cercanos, (2) selecciona aleatoriamente uno de esos vecinos, (3) crea un nuevo ejemplo en un punto aleatorio de la l√≠nea que conecta ambos. Esto aumenta la diversidad sin simplemente duplicar ejemplos.

**Analog√≠a**: Es como crear nuevos colores mezclando dos colores existentes. Si tienes azul y verde, puedes crear turquesa, aguamarina, etc. No est√°s copiando colores, est√°s creando nuevos que est√°n "entre" los originales.

**Mini-glosario**:

- **Interpolaci√≥n**: Crear valores intermedios entre dos puntos
- **K-vecinos**: Los k ejemplos m√°s cercanos a un punto
- **Ejemplo sint√©tico**: Dato artificial creado por el algoritmo

---

## Pregunta 13
**¬øPor qu√© no simplemente duplicar los ejemplos de la clase minoritaria?**

**Respuesta**: Duplicar ejemplos (oversampling simple) causa overfitting porque el modelo memoriza los ejemplos repetidos en lugar de aprender patrones generales. SMOTE evita esto creando ejemplos nuevos que son similares pero no id√©nticos a los originales, lo que ayuda al modelo a generalizar mejor a datos no vistos.

**Analog√≠a**: Es como estudiar para un examen. Si solo memorizas las mismas 10 preguntas repetidas, fallar√°s cuando aparezca una pregunta nueva. Pero si estudias variaciones de esas preguntas, estar√°s mejor preparado.

**Mini-glosario**:

- **Overfitting**: Modelo que memoriza en lugar de generalizar
- **Generalizaci√≥n**: Capacidad de funcionar bien con datos nuevos
- **Varianza**: Sensibilidad del modelo a cambios en los datos

---

## Pregunta 14
**¬øCu√°ndo se aplica el balanceo: antes o despu√©s de dividir los datos?**

**Respuesta**: El balanceo SIEMPRE se aplica DESPU√âS de dividir los datos, y SOLO al conjunto de entrenamiento. Aplicarlo antes causar√≠a data leakage porque los ejemplos sint√©ticos del test podr√≠an estar basados en ejemplos del train. En el proyecto, primero se hizo train_test_split y luego se aplic√≥ SMOTE solo a X_train.

**Analog√≠a**: Es como preparar un examen: no puedes usar las respuestas del examen final para estudiar. El conjunto de test debe permanecer "puro" y representar datos del mundo real.

**Mini-glosario**:

- **Data leakage**: Contaminaci√≥n del test con informaci√≥n del train
- **Conjunto de validaci√≥n**: Datos separados para evaluar el modelo
- **Integridad de datos**: Mantener la separaci√≥n train/test

---

# ü§ñ CATEGOR√çA 4: MODELADO Y ALGORITMOS

## Pregunta 15
**¬øQu√© algoritmos de Machine Learning evaluaste y por qu√© esos espec√≠ficamente?**

**Respuesta**: Se evaluaron 7 algoritmos:

1. **Logistic Regression**: Baseline simple e interpretable
2. **Decision Tree**: F√°cil de interpretar, captura no linealidades
3. **Random Forest**: Ensemble de √°rboles, reduce overfitting
4. **Gradient Boosting**: Ensemble secuencial, muy potente
5. **XGBoost**: Versi√≥n optimizada de Gradient Boosting
6. **SVM**: Bueno para espacios de alta dimensi√≥n
7. **KNN**: Simple, basado en similitud

Se eligieron para cubrir diferentes familias de algoritmos y encontrar el mejor para este problema espec√≠fico.

**Analog√≠a**: Es como probar diferentes herramientas para un trabajo: martillo, destornillador, llave inglesa. Cada una tiene sus fortalezas, y solo prob√°ndolas sabes cu√°l funciona mejor para tu tarea.

**Mini-glosario**:

- **Ensemble**: Combinaci√≥n de m√∫ltiples modelos
- **Baseline**: Modelo simple de referencia
- **Hiperpar√°metro**: Configuraci√≥n del algoritmo

---

## Pregunta 16
**¬øPor qu√© Logistic Regression Optimizado fue el mejor modelo?**

**Respuesta**: Logistic Regression Optimizado obtuvo el mejor ROC-AUC (0.8503) despu√©s de la optimizaci√≥n porque: (1) es un modelo lineal robusto e interpretable, (2) con la t√©cnica de balanceo Undersampling mejora significativamente el Recall (79.41%), (3) es computacionalmente eficiente, (4) los coeficientes permiten interpretar la importancia de cada feature, (5) mostr√≥ consistencia en la validaci√≥n de robustez con m√∫ltiples semillas (ROC-AUC promedio: 0.8513 ¬± 0.0065).

**Analog√≠a**: Es como un juez experimentado que, aunque usa reglas simples y claras, logra tomar las mejores decisiones cuando se le da la informaci√≥n correctamente balanceada.

**Mini-glosario**:

- **Boosting**: T√©cnica que combina modelos d√©biles secuencialmente
- **Residuos**: Errores que el modelo anterior no pudo predecir
- **Regularizaci√≥n**: T√©cnica para prevenir overfitting

---

## Pregunta 17
**¬øQu√© es la validaci√≥n cruzada y por qu√© la usaste?**

**Respuesta**: La validaci√≥n cruzada (CV) divide los datos de entrenamiento en k partes (folds), entrena k modelos usando k-1 partes y valida con la restante, rotando. Se us√≥ StratifiedKFold con 5 folds para obtener una estimaci√≥n m√°s robusta del rendimiento. El CV score promedio fue 0.84, similar al score en test, indicando que el modelo generaliza bien.

**Analog√≠a**: Es como un estudiante que practica con 5 ex√°menes de prueba diferentes antes del examen real. Si obtiene notas similares en todos, sabes que realmente aprendi√≥ y no solo memoriz√≥ un examen espec√≠fico.

**Mini-glosario**:

- **Cross-validation**: T√©cnica de validaci√≥n con m√∫ltiples divisiones
- **Fold**: Cada partici√≥n de los datos en CV
- **StratifiedKFold**: CV que mantiene proporciones de clases

---

## Pregunta 18
**¬øQu√© es RandomizedSearchCV y c√≥mo lo usaste para optimizar hiperpar√°metros?**

**Respuesta**: RandomizedSearchCV busca la mejor combinaci√≥n de hiperpar√°metros probando combinaciones aleatorias de un espacio definido. Se definieron rangos para par√°metros como n_estimators (50-300), max_depth (3-10), learning_rate (0.01-0.3), etc. Se probaron 50 combinaciones con 5-fold CV, optimizando ROC-AUC. Esto es m√°s eficiente que GridSearch cuando el espacio de b√∫squeda es grande.

**Analog√≠a**: Es como buscar el mejor restaurante en una ciudad. GridSearch visitar√≠a TODOS los restaurantes; RandomizedSearch visita una muestra aleatoria representativa y encuentra uno excelente en menos tiempo.

**Mini-glosario**:

- **Hiperpar√°metro**: Configuraci√≥n externa del modelo
- **Grid Search**: B√∫squeda exhaustiva de todas las combinaciones
- **Espacio de b√∫squeda**: Rango de valores a explorar

---

## Pregunta 19
**¬øCu√°les fueron los mejores hiperpar√°metros encontrados para Logistic Regression?**

**Respuesta**: Los hiperpar√°metros √≥ptimos encontrados fueron:

- C: valor √≥ptimo de regularizaci√≥n inversa
- penalty: tipo de regularizaci√≥n (L1 o L2)
- solver: algoritmo de optimizaci√≥n (liblinear o saga)
- max_iter: iteraciones m√°ximas para convergencia
- learning_rate: ~0.1 (tasa de aprendizaje)
- min_samples_split: ~10 (m√≠nimo para dividir)
- min_samples_leaf: ~4 (m√≠nimo en hojas)
- subsample: ~0.8 (fracci√≥n de datos por √°rbol)

Estos valores balancean complejidad y generalizaci√≥n.

**Analog√≠a**: Es como afinar un instrumento musical: cada perilla (hiperpar√°metro) afecta el sonido final, y hay una combinaci√≥n √≥ptima que produce la mejor melod√≠a.

**Mini-glosario**:

- **n_estimators**: N√∫mero de √°rboles en el ensemble
- **learning_rate**: Cu√°nto aprende cada √°rbol nuevo
- **max_depth**: Complejidad m√°xima de cada √°rbol

---

# üìà CATEGOR√çA 5: M√âTRICAS DE EVALUACI√ìN

## Pregunta 20
**¬øPor qu√© usaste ROC-AUC como m√©trica principal en lugar de Accuracy?**

**Respuesta**: Accuracy es enga√±osa con clases desbalanceadas: un modelo que predice siempre "No Churn" tendr√≠a 73% accuracy pero ser√≠a in√∫til. ROC-AUC mide la capacidad del modelo para distinguir entre clases en todos los umbrales de decisi√≥n, siendo m√°s robusta al desbalance. Un ROC-AUC de 0.8503 significa que hay 85% de probabilidad de que el modelo rankee correctamente un caso positivo sobre uno negativo.

**Analog√≠a**: Es como evaluar un detector de metales en un aeropuerto. No importa cu√°ntas personas "normales" deja pasar (accuracy); importa que detecte las armas cuando las hay (ROC-AUC).

**Mini-glosario**:

- **ROC-AUC**: √Årea bajo la curva ROC (0.5 = aleatorio, 1.0 = perfecto)
- **Umbral de decisi√≥n**: Punto de corte para clasificar positivo/negativo
- **True Positive Rate**: Proporci√≥n de positivos correctamente identificados

---

## Pregunta 21
**¬øQu√© es la matriz de confusi√≥n y c√≥mo la interpretas?**

**Respuesta**: La matriz de confusi√≥n muestra los 4 resultados posibles:

- **TN (True Negative)**: Predijo No Churn, era No Churn ‚úì
- **FP (False Positive)**: Predijo Churn, era No Churn ‚úó
- **FN (False Negative)**: Predijo No Churn, era Churn ‚úó (el m√°s costoso)
- **TP (True Positive)**: Predijo Churn, era Churn ‚úì

En el modelo: TN=~850, FP=~180, FN=~75, TP=~300. Los FN son cr√≠ticos porque son clientes que abandonar√°n sin que los detectemos.

**Analog√≠a**: Es como un diagn√≥stico m√©dico: TN = sano diagnosticado sano, FP = sano diagnosticado enfermo (susto innecesario), FN = enfermo diagnosticado sano (peligroso), TP = enfermo diagnosticado enfermo.

**Mini-glosario**:

- **Verdadero/Falso**: Si la predicci√≥n fue correcta o no
- **Positivo/Negativo**: La clase predicha
- **Matriz de confusi√≥n**: Tabla 2x2 de resultados

---

## Pregunta 22
**¬øCu√°l es la diferencia entre Precision y Recall, y cu√°l es m√°s importante en este problema?**

**Respuesta**: 

- **Precision**: De los que predije como Churn, ¬øcu√°ntos realmente lo son? (TP/(TP+FP)) = ~52%
- **Recall**: De los que realmente son Churn, ¬øcu√°ntos detect√©? (TP/(TP+FN)) = ~80%

En churn prediction, **Recall es m√°s importante** porque el costo de no detectar un cliente que abandonar√° (FN) es mayor que el costo de contactar a uno que no iba a abandonar (FP). Preferimos "molestar" a algunos clientes leales que perder clientes en riesgo.

**Analog√≠a**: En un detector de incendios, preferimos falsas alarmas (bajo precision) a no detectar un incendio real (bajo recall). El costo de un incendio no detectado es catastr√≥fico.

**Mini-glosario**:

- **Precision**: Exactitud de las predicciones positivas
- **Recall (Sensibilidad)**: Cobertura de los casos positivos reales
- **F1-Score**: Media arm√≥nica de Precision y Recall

---

## Pregunta 23
**¬øQu√© es el F1-Score y cu√°ndo es √∫til?**

**Respuesta**: El F1-Score es la media arm√≥nica de Precision y Recall: F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall). En el modelo, F1 = ~0.63. Es √∫til cuando necesitas un balance entre ambas m√©tricas y cuando las clases est√°n desbalanceadas. La media arm√≥nica penaliza valores extremos, as√≠ que un F1 alto requiere que ambas m√©tricas sean razonablemente buenas.

**Analog√≠a**: Es como la nota final de un curso que requiere aprobar tanto teor√≠a como pr√°ctica. No puedes compensar un 0 en pr√°ctica con un 10 en teor√≠a; necesitas un balance.

**Mini-glosario**:

- **Media arm√≥nica**: Tipo de promedio que penaliza valores bajos
- **Trade-off**: Compromiso entre dos objetivos opuestos
- **Balance**: Equilibrio entre Precision y Recall

---

## Pregunta 24
**¬øC√≥mo validaste la robustez del modelo antes de producci√≥n?**

**Respuesta**: Se realiz√≥ validaci√≥n de robustez con 5 semillas diferentes (42, 123, 456, 789, 2024), entrenando y evaluando el modelo con cada una. Los criterios de aceptaci√≥n fueron:

1. Desviaci√≥n est√°ndar de ROC-AUC < 0.02 ‚úì (obtenido: 0.0065)
2. Rango de variaci√≥n < 0.05 ‚úì (obtenido: 0.0163)
3. ROC-AUC promedio > 0.80 ‚úì (obtenido: 0.8513)

El modelo pas√≥ todos los criterios, indicando estabilidad para producci√≥n.

**Analog√≠a**: Es como probar un carro en diferentes condiciones (lluvia, sol, noche, d√≠a) antes de venderlo. Si funciona bien en todas, puedes confiar en que funcionar√° para el cliente.

**Mini-glosario**:

- **Robustez**: Estabilidad del modelo ante variaciones
- **Semilla aleatoria**: Valor que controla la aleatoriedad
- **Criterios de aceptaci√≥n**: Umbrales m√≠nimos para producci√≥n

---

# üí° CATEGOR√çA 6: INTERPRETABILIDAD Y FEATURE IMPORTANCE

## Pregunta 25
**¬øCu√°les son las 5 variables m√°s importantes para predecir churn?**

**Respuesta**: Las 5 variables m√°s importantes seg√∫n el modelo son:

1. **Contract_Month-to-month** (~25%): Contratos mensuales tienen alto riesgo
2. **tenure** (~18%): Antig√ºedad del cliente
3. **TotalCharges** (~12%): Monto total pagado
4. **MonthlyCharges** (~10%): Cargo mensual
5. **InternetService_Fiber optic** (~8%): Servicio de fibra √≥ptica

Estas variables explican m√°s del 70% de la capacidad predictiva del modelo.

**Analog√≠a**: Es como identificar los factores principales de √©xito acad√©mico: horas de estudio, asistencia a clase, calidad del sue√±o. Algunos factores pesan m√°s que otros.

**Mini-glosario**:

- **Feature Importance**: Peso de cada variable en las predicciones
- **Importancia relativa**: Porcentaje de contribuci√≥n de cada variable
- **Variables predictoras**: Caracter√≠sticas usadas para predecir

---

## Pregunta 26
**¬øPor qu√© el tipo de contrato es la variable m√°s importante?**

**Respuesta**: Los contratos mes a mes tienen 42% de churn vs 11% en contratos anuales y 3% en bianuales. Esto se debe a que: (1) no hay penalizaci√≥n por cancelar, (2) indica menor compromiso inicial del cliente, (3) facilita la comparaci√≥n con competidores. El modelo aprende que este es el predictor m√°s fuerte de abandono.

**Analog√≠a**: Es como la diferencia entre alquilar y comprar una casa. El inquilino (mes a mes) puede irse f√°cilmente; el propietario (contrato largo) tiene m√°s razones para quedarse.

**Mini-glosario**:

- **Contrato mes a mes**: Sin compromiso a largo plazo
- **Lock-in**: Efecto de retenci√≥n por compromiso contractual
- **Costo de cambio**: Barreras para cambiar de proveedor

---

## Pregunta 27
**¬øC√≥mo explicar√≠as las predicciones del modelo a un ejecutivo no t√©cnico?**

**Respuesta**: El modelo analiza el perfil de cada cliente y asigna una probabilidad de abandono (0-100%). Los factores principales son: tipo de contrato, antig√ºedad, servicios contratados y montos pagados. Por ejemplo: "Este cliente tiene 85% de probabilidad de abandonar porque tiene contrato mensual, solo 3 meses de antig√ºedad, y no tiene servicios de seguridad online. Recomendamos ofrecerle un descuento por contrato anual."

**Analog√≠a**: Es como un sem√°foro de riesgo: verde (bajo riesgo), amarillo (atenci√≥n), rojo (acci√≥n urgente). El modelo nos dice el color y por qu√©.

**Mini-glosario**:

- **Probabilidad de churn**: Riesgo estimado de abandono (0-1)
- **Scoring**: Asignaci√≥n de puntuaci√≥n de riesgo
- **Interpretabilidad**: Capacidad de explicar las predicciones

---

# üí∞ CATEGOR√çA 7: IMPACTO DE NEGOCIO Y ROI

## Pregunta 28
**¬øC√≥mo calculaste el ROI del modelo de predicci√≥n de churn?**

**Respuesta**: Se us√≥ la funci√≥n `reporte_negocio()` con par√°metros:

- LTV (Lifetime Value) por cliente: $2,000
- Costo de campa√±a de retenci√≥n: $150
- Tasa de √©xito de retenci√≥n: 50%

ROI = (Clientes retenidos √ó LTV - Costo total de campa√±as) / Costo total √ó 100

Con ~300 TP y 50% de √©xito, se retienen ~150 clientes, generando $300,000 en valor vs ~$72,000 en costos = ROI de ~316%.

**Analog√≠a**: Es como invertir en publicidad: gastas $1 y recuperas $4. El modelo te dice d√≥nde invertir ese d√≥lar para m√°ximo retorno.

**Mini-glosario**:

- **ROI**: Return on Investment (retorno sobre inversi√≥n)
- **LTV**: Valor total del cliente durante su vida √∫til
- **Costo de adquisici√≥n**: Gasto para conseguir un nuevo cliente

---

## Pregunta 29
**¬øCu√°l es el costo de un Falso Negativo vs un Falso Positivo en este contexto?**

**Respuesta**: 

- **Falso Negativo (FN)**: No detectar un cliente que abandonar√°. Costo = LTV perdido = $2,000
- **Falso Positivo (FP)**: Contactar a un cliente que no iba a abandonar. Costo = Costo de campa√±a = $150

El FN es ~13 veces m√°s costoso que el FP. Por eso optimizamos para alto Recall aunque sacrifiquemos algo de Precision.

**Analog√≠a**: En medicina, no detectar un c√°ncer (FN) es mucho peor que hacer una biopsia innecesaria (FP). El costo del error no es sim√©trico.

**Mini-glosario**:

- **Costo asim√©trico**: Cuando los errores tienen diferentes consecuencias
- **Matriz de costos**: Asignaci√≥n de costos a cada tipo de error
- **Optimizaci√≥n por costo**: Minimizar el costo total de errores

---

## Pregunta 30
**¬øQu√© estrategias de retenci√≥n recomendar√≠as bas√°ndote en los insights del modelo?**

**Respuesta**: Bas√°ndome en las variables importantes:

1. **Migraci√≥n a contratos largos**: Ofrecer descuentos por contratos anuales/bianuales
2. **Programa de onboarding**: Atenci√≥n especial en primeros 12 meses
3. **Bundles de servicios**: Promover OnlineSecurity (seguridad en l√≠nea), TechSupport (soporte t√©cnico) - reducen churn
4. **Revisi√≥n de precios**: Analizar clientes con cargos altos vs valor percibido
5. **Retenci√≥n proactiva**: Contactar clientes con probabilidad >70% antes de que decidan irse

**Analog√≠a**: Es como un programa de fidelizaci√≥n de aerol√≠nea: millas, upgrades, atenci√≥n preferencial. Cada beneficio ataca un factor de riesgo espec√≠fico.

**Mini-glosario**:

- **Estrategia de retenci√≥n**: Acciones para evitar el abandono
- **Onboarding**: Proceso de integraci√≥n de nuevos clientes
- **Bundle**: Paquete de servicios combinados

---

# üî¨ CATEGOR√çA 8: ASPECTOS T√âCNICOS AVANZADOS

## Pregunta 31
**¬øQu√© es el overfitting y c√≥mo lo previniste?**

**Respuesta**: Overfitting ocurre cuando el modelo memoriza los datos de entrenamiento pero falla con datos nuevos. Se previno mediante:

1. **Validaci√≥n cruzada**: Evaluar en m√∫ltiples particiones
2. **Regularizaci√≥n**: Par√°metros como min_samples_leaf, max_depth
3. **Early stopping impl√≠cito**: Limitar n_estimators
4. **Validaci√≥n de robustez**: Probar con m√∫ltiples semillas
5. **Comparar train vs test score**: Diferencia peque√±a indica buen ajuste

El modelo tiene CV score ~0.84 y test score ~0.85, indicando buena generalizaci√≥n.

**Analog√≠a**: Es como un estudiante que memoriza respuestas vs uno que entiende conceptos. El primero falla con preguntas nuevas; el segundo puede resolver problemas que nunca vio.

**Mini-glosario**:

- **Overfitting**: Sobreajuste a los datos de entrenamiento
- **Underfitting**: Modelo demasiado simple
- **Generalizaci√≥n**: Rendimiento en datos no vistos

---

## Pregunta 32
**¬øPor qu√© usaste random_state=42 en todo el proyecto?**

**Respuesta**: random_state=42 es una semilla que controla la aleatoriedad, garantizando reproducibilidad. Esto significa que cualquier persona que ejecute el c√≥digo obtendr√° exactamente los mismos resultados. Es esencial para: (1) debugging, (2) comparaci√≥n justa de modelos, (3) documentaci√≥n cient√≠fica, (4) auditor√≠a. El 42 es una convenci√≥n popular (referencia a "The Hitchhiker's Guide to the Galaxy").

**Analog√≠a**: Es como usar la misma receta con las mismas medidas exactas. Si cambias algo, el resultado puede variar, pero con la misma receta siempre obtienes el mismo plato.

**Mini-glosario**:

- **Reproducibilidad**: Capacidad de obtener los mismos resultados
- **Semilla aleatoria**: Valor inicial para generador de n√∫meros aleatorios
- **Determinismo**: Mismo input produce mismo output

---

## Pregunta 33
**¬øC√≥mo guardar√≠as el modelo para usarlo en producci√≥n?**

**Respuesta**: Se us√≥ `joblib.dump()` para guardar:

1. **churn_model.pkl**: El modelo entrenado (~0.5 MB)
2. **preprocessor.pkl**: El pipeline de preprocesamiento (~0.1 MB)
3. **metadata.json**: Informaci√≥n del modelo, m√©tricas, versiones

Para predecir en producci√≥n: cargar ambos archivos, aplicar preprocessor.transform() a los datos nuevos, y luego model.predict_proba(). El tama√±o total (~0.6 MB) es adecuado para deployment en Render/Railway.

**Analog√≠a**: Es como guardar una receta completa: los ingredientes (preprocessor), las instrucciones (modelo), y las notas del chef (metadata). Cualquiera puede reproducir el plato.

**Mini-glosario**:

- **Serializaci√≥n**: Convertir objeto a archivo
- **Pickle/Joblib**: Formatos para guardar objetos Python
- **Deployment**: Poner el modelo en producci√≥n

---

## Pregunta 34
**¬øQu√© consideraciones tendr√≠as para monitorear el modelo en producci√≥n?**

**Respuesta**: Monitoreo esencial incluye:

1. **Data drift**: ¬øLos datos nuevos son similares a los de entrenamiento?
2. **Concept drift**: ¬øLa relaci√≥n entre variables y churn cambi√≥?
3. **M√©tricas en vivo**: Comparar predicciones vs resultados reales
4. **Latencia**: Tiempo de respuesta de las predicciones
5. **Volumen**: N√∫mero de predicciones por per√≠odo
6. **Alertas**: Notificar si m√©tricas caen bajo umbrales

Recomendaci√≥n: reentrenar mensualmente o cuando las m√©tricas degraden >5%.

**Analog√≠a**: Es como el mantenimiento de un carro: revisiones peri√≥dicas, alertas del tablero, y reparaciones cuando algo falla. No esperas a que se descomponga.

**Mini-glosario**:

- **Data drift**: Cambio en la distribuci√≥n de los datos
- **Concept drift**: Cambio en la relaci√≥n datos-resultado
- **MLOps**: Operaciones de Machine Learning en producci√≥n

---

# üéØ CATEGOR√çA 9: PREGUNTAS DE S√çNTESIS Y REFLEXI√ìN

## Pregunta 35
**Si tuvieras que explicar todo el proyecto en 2 minutos, ¬øqu√© dir√≠as?**

**Respuesta**: "Desarrollamos un modelo de Machine Learning para predecir qu√© clientes de telecomunicaciones abandonar√°n el servicio. Analizamos 7,043 clientes con 21 caracter√≠sticas. El principal desaf√≠o fue el desbalance de clases (27% churn), que resolvimos con Undersampling. Evaluamos 7 algoritmos y Logistic Regression Optimizado fue el mejor con 85% ROC-AUC y 79% Recall. Las variables m√°s importantes son tipo de contrato, antig√ºedad y cargos. El modelo puede generar ROI de 300%+ al permitir campa√±as de retenci√≥n focalizadas. Est√° validado, es robusto, y listo para producci√≥n."

**Analog√≠a**: Es como el elevator pitch de una startup: problema, soluci√≥n, resultados, en el tiempo que dura un viaje en ascensor.

**Mini-glosario**:

- **Elevator pitch**: Presentaci√≥n concisa de un proyecto
- **Key takeaways**: Puntos principales a recordar
- **Executive summary**: Resumen para tomadores de decisiones

---

## Pregunta 36
**¬øCu√°les fueron los principales desaf√≠os t√©cnicos y c√≥mo los resolviste?**

**Respuesta**: Los principales desaf√≠os fueron:

1. **Desbalance de clases**: Resuelto con Undersampling (seleccionado tras comparar SMOTE, SMOTE+Tomek y Undersampling)
2. **Valores faltantes**: Imputaci√≥n l√≥gica basada en contexto (tenure=0)
3. **Variables mixtas**: Pipeline con ColumnTransformer para diferentes tipos
4. **Selecci√≥n de modelo**: Evaluaci√≥n sistem√°tica de 7 algoritmos
5. **Optimizaci√≥n**: RandomizedSearchCV para hiperpar√°metros
6. **Validaci√≥n**: M√∫ltiples semillas para garantizar robustez

Cada desaf√≠o se abord√≥ con metodolog√≠a rigurosa y documentaci√≥n.

**Analog√≠a**: Es como escalar una monta√±a: cada obst√°culo (grieta, tormenta, altitud) requiere una t√©cnica espec√≠fica. La preparaci√≥n y el equipo adecuado son clave.

**Mini-glosario**:

- **Desaf√≠o t√©cnico**: Problema que requiere soluci√≥n especializada
- **Metodolog√≠a**: Proceso sistem√°tico para resolver problemas
- **Best practices**: Mejores pr√°cticas de la industria

---

## Pregunta 37
**¬øQu√© har√≠as diferente si empezaras el proyecto de nuevo?**

**Respuesta**: Mejoras potenciales:

1. **M√°s Feature Engineering**: Crear variables de tendencia temporal
2. **An√°lisis de cohortes**: Segmentar por fecha de adquisici√≥n
3. **Modelos de supervivencia**: Predecir cu√°ndo abandonar√°, no solo si
4. **Explicabilidad**: Implementar SHAP values para interpretaci√≥n local
5. **A/B testing**: Dise√±ar experimento para validar impacto real
6. **Pipeline automatizado**: MLflow para tracking de experimentos

El proyecto actual es s√≥lido, pero siempre hay espacio para mejora.

**Analog√≠a**: Es como renovar una casa: la estructura es buena, pero podr√≠as mejorar la cocina, agregar un ba√±o, o modernizar la electricidad.

**Mini-glosario**:

- **An√°lisis de cohortes**: Estudiar grupos por per√≠odo de ingreso
- **SHAP values**: Explicaci√≥n de predicciones individuales
- **MLflow**: Herramienta para gesti√≥n de experimentos ML

---

## Pregunta 38
**¬øC√≥mo manejar√≠as datos nuevos que tienen categor√≠as que no exist√≠an en el entrenamiento?**

**Respuesta**: Este es el problema de "categor√≠as no vistas". Soluciones:

1. **handle_unknown='ignore'** en OneHotEncoder: Ignora categor√≠as nuevas
2. **Categor√≠a "Otros"**: Agrupar categor√≠as raras durante entrenamiento
3. **Reentrenamiento**: Incluir nuevas categor√≠as peri√≥dicamente
4. **Validaci√≥n de entrada**: Alertar cuando aparecen valores inesperados
5. **Fallback**: Usar predicci√≥n por defecto si hay error

En el proyecto se us√≥ handle_unknown='ignore' para robustez.

**Analog√≠a**: Es como un traductor que encuentra una palabra nueva: puede ignorarla, usar una aproximaci√≥n, o pedir ayuda. Lo importante es no fallar completamente.

**Mini-glosario**:

- **Categor√≠a no vista**: Valor que no exist√≠a en entrenamiento
- **Handle unknown**: Estrategia para manejar valores nuevos
- **Robustez de entrada**: Tolerancia a datos inesperados

---

## Pregunta 39
**¬øC√≥mo comunicar√≠as los resultados a diferentes audiencias?**

**Respuesta**: Adaptaci√≥n por audiencia:

- **Ejecutivos**: ROI, impacto en ingresos, recomendaciones de acci√≥n
- **Marketing**: Segmentos de riesgo, variables accionables, campa√±as sugeridas
- **T√©cnicos**: M√©tricas detalladas, arquitectura, c√≥digo, reproducibilidad
- **Operaciones**: C√≥mo usar el modelo, qu√© datos necesita, frecuencia de actualizaci√≥n

Cada audiencia tiene diferentes necesidades y nivel de detalle.

**Analog√≠a**: Es como explicar el clima: a un piloto le das datos t√©cnicos, a un turista le dices "lleva paraguas", a un agricultor le hablas de precipitaci√≥n acumulada.

**Mini-glosario**:

- **Stakeholder**: Persona interesada en el proyecto
- **Comunicaci√≥n t√©cnica**: Adaptada a expertos
- **Comunicaci√≥n ejecutiva**: Enfocada en impacto y decisiones

---

## Pregunta 40
**¬øCu√°l es el siguiente paso despu√©s de este proyecto?**

**Respuesta**: Roadmap sugerido:

1. **Corto plazo (1-2 semanas)**: Desarrollar API REST con Flask/FastAPI
2. **Medio plazo (1 mes)**: Dashboard interactivo con Streamlit
3. **Producci√≥n (2 meses)**: Deploy en cloud (Render/Railway/AWS)
4. **Integraci√≥n (3 meses)**: Conectar con CRM de la empresa
5. **Automatizaci√≥n (6 meses)**: Pipeline de reentrenamiento autom√°tico
6. **Expansi√≥n**: Modelos de upselling, cross-selling, lifetime value

El modelo actual es el MVP; el valor real viene de la implementaci√≥n y uso continuo.

**Analog√≠a**: Es como construir una casa: el proyecto arquitect√≥nico (modelo) est√° listo, ahora viene la construcci√≥n (deployment), la mudanza (integraci√≥n), y el mantenimiento (monitoreo).

**Mini-glosario**:

- **MVP**: Minimum Viable Product (producto m√≠nimo viable)
- **Roadmap**: Plan de desarrollo a futuro
- **API REST**: Interfaz para consumir el modelo program√°ticamente

---

# üìö GLOSARIO GENERAL

| T√©rmino | Definici√≥n |
|---------|------------|
| **Churn** | Abandono de clientes |
| **ROC-AUC** | M√©trica de capacidad discriminativa (0.5-1.0) |
| **Undersampling** | T√©cnica de reducci√≥n de clase mayoritaria |
| **SMOTE** | T√©cnica de oversampling sint√©tico |
| **Logistic Regression** | Modelo lineal de clasificaci√≥n, interpretable |
| **Feature Engineering** | Creaci√≥n de nuevas variables |
| **Cross-validation** | Validaci√≥n con m√∫ltiples particiones |
| **Precision** | Exactitud de predicciones positivas |
| **Recall** | Cobertura de casos positivos reales |
| **Overfitting** | Sobreajuste a datos de entrenamiento |
| **Pipeline** | Secuencia de transformaciones |
| **Hiperpar√°metro** | Configuraci√≥n externa del modelo |
| **LTV** | Lifetime Value (valor del cliente) |
| **ROI** | Return on Investment |
| **Data drift** | Cambio en distribuci√≥n de datos |
| **Deployment** | Puesta en producci√≥n |

---

*Documento generado para la sustentaci√≥n del proyecto de predicci√≥n de Customer Churn*
*Bootcamp de IA - Nivel B√°sico*

