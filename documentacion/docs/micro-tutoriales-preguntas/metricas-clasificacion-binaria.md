# üìä Explicaci√≥n de las M√©tricas de Clasificaci√≥n Binaria en tu Notebook

Te explico c√≥mo funcionan las m√©tricas de clasificaci√≥n binaria que utilizas en `Telco_Customer_Churn.ipynb`:

## üéØ **1. M√©tricas Importadas**

En tu notebook importas estas m√©tricas de scikit-learn:

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve,
    precision_recall_curve, average_precision_score
)
```

---

## üìà **2. Las 5 M√©tricas Principales**

### **üéØ Accuracy (Exactitud)**
- **¬øQu√© mide?** Porcentaje total de predicciones correctas
- **F√≥rmula:** `(VP + VN) / Total`
- **En tu c√≥digo:**

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy:.4f}")
```

**‚ö†Ô∏è Limitaci√≥n:** No es ideal para datos desbalanceados (como tu caso de churn)

---

### **üéØ Precision (Precisi√≥n)**
- **¬øQu√© mide?** De todos los que predijimos como "churn", ¬øcu√°ntos realmente hicieron churn?
- **F√≥rmula:** `VP / (VP + FP)`
- **Analog√≠a:** Si env√≠as 100 ofertas de retenci√≥n, ¬øcu√°ntas fueron a clientes que realmente iban a irse?

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
precision = precision_score(y_test, y_pred)
print(f"  Precision: {precision:.4f}")
```

**üí° En tu modelo:** 72% significa que de cada 100 clientes que predices como "churn", 72 realmente se van.

---

### **üéØ Recall (Sensibilidad/Exhaustividad)**
- **¬øQu√© mide?** De todos los que realmente hicieron churn, ¬øcu√°ntos detectamos?
- **F√≥rmula:** `VP / (VP + FN)`
- **Analog√≠a:** De todos los clientes que se fueron, ¬øa cu√°ntos detectaste a tiempo?

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
recall = recall_score(y_test, y_pred)
print(f"  Recall: {recall:.4f}")
```

**üí° En tu modelo:** 83% significa que detectas 83 de cada 100 clientes que realmente se van.

**üî• Importancia:** Esta es la m√©trica M√ÅS IMPORTANTE en churn porque no queremos perder clientes en riesgo.

---

### **üéØ F1-Score**
- **¬øQu√© mide?** Balance arm√≥nico entre Precision y Recall
- **F√≥rmula:** `2 √ó (Precision √ó Recall) / (Precision + Recall)`
- **Cu√°ndo usarlo:** Cuando necesitas equilibrio entre precision y recall

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
f1 = f1_score(y_test, y_pred)
print(f"  F1-Score: {f1:.4f}")
```

**üí° En tu modelo:** 77% indica un buen balance entre detectar churners y no molestar a clientes leales.

---

### **üéØ ROC-AUC (Area Under the ROC Curve)**
- **¬øQu√© mide?** Capacidad del modelo para discriminar entre clases
- **Rango:** 0.5 (aleatorio) a 1.0 (perfecto)
- **Ventaja:** Independiente del umbral de decisi√≥n

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"  ROC-AUC: {roc_auc:.4f}")
```

**üí° En tu modelo:** 0.87 es EXCELENTE (>0.8 se considera muy bueno)

---

## üìä **3. C√≥mo se Calculan en tu C√≥digo**

Tu notebook calcula las m√©tricas en 3 momentos:

### **A) Modelos Baseline (sin balanceo)**
```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
for name, model in models.items():
    model.fit(X_train_processed, y_train)
    y_pred = model.predict(X_test_processed)
    y_pred_proba = model.predict_proba(X_test_processed)[:, 1]

    # Calcular todas las m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
```

### **B) Modelos con SMOTE (datos balanceados)**
Repite el mismo proceso pero con datos balanceados para mejorar el recall.

### **C) Modelo Final Optimizado**
```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
print(f"  Accuracy: {accuracy_score(y_test, y_pred_best):.4f}")
print(f"  Precision: {precision_score(y_test, y_pred_best):.4f}")
print(f"  Recall: {recall_score(y_test, y_pred_best):.4f}")
print(f"  F1-Score: {f1_score(y_test, y_pred_best):.4f}")
print(f"  ROC-AUC: {roc_auc_score(y_test, y_pred_proba_best):.4f}")
```

---

## üìâ **4. Visualizaciones de M√©tricas**

### **A) Matriz de Confusi√≥n**
Muestra los 4 tipos de predicciones:

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Churn', 'Churn'],
            yticklabels=['No Churn', 'Churn'])
```

**Interpretaci√≥n:**

- **Verdaderos Negativos (VN):** Clientes que NO se fueron y predijimos correctamente
- **Falsos Positivos (FP):** Clientes leales que marcamos como "en riesgo" ‚ùå
- **Falsos Negativos (FN):** Clientes que se fueron pero NO detectamos ‚ùå‚ùå (¬°Lo peor!)
- **Verdaderos Positivos (VP):** Clientes en riesgo que detectamos correctamente ‚úÖ

---

### **B) Curva ROC**
Muestra el trade-off entre True Positive Rate y False Positive Rate:

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)
roc_auc = roc_auc_score(y_test, y_pred_proba_best)

ax2.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC curve (AUC = {roc_auc:.4f})')
ax2.plot([0, 1], [0, 1], color='navy', lw=2,
         linestyle='--', label='Random Classifier')
```

**Interpretaci√≥n:**

- L√≠nea diagonal = clasificador aleatorio (AUC = 0.5)
- Cuanto m√°s cerca de la esquina superior izquierda, mejor
- Tu AUC = 0.87 significa que el modelo es muy bueno discriminando

---

### **C) Curva Precision-Recall**
Especialmente √∫til para datos desbalanceados:

```python
path=Telco_Customer_Churn.ipynb mode=EXCERPT
precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_best)
avg_precision = average_precision_score(y_test, y_pred_proba_best)

ax3.plot(recall_curve, precision_curve, color='green', lw=2,
         label=f'PR curve (AP = {avg_precision:.4f})')
```

---

## üéØ **5. ¬øPor Qu√© Estas M√©tricas en Churn?**

| M√©trica | Importancia en Churn | Raz√≥n |
|---------|---------------------|-------|
| **Recall** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | No podemos perder clientes en riesgo |
| **ROC-AUC** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Mide capacidad general del modelo |
| **F1-Score** | ‚≠ê‚≠ê‚≠ê‚≠ê | Balance entre detectar churn y no molestar clientes |
| **Precision** | ‚≠ê‚≠ê‚≠ê | Evita gastar en campa√±as innecesarias |
| **Accuracy** | ‚≠ê‚≠ê | Enga√±osa con datos desbalanceados |

---

## üí° **6. Resultados de tu Modelo**

Seg√∫n tu metadata guardada:

```python
'metrics': {
    'roc_auc': 0.87,    # ‚≠ê Excelente discriminaci√≥n
    'recall': 0.83,     # ‚≠ê Detectas 83% de churners
    'precision': 0.72,  # ‚úÖ 72% de tus alertas son correctas
    'f1_score': 0.77    # ‚úÖ Buen balance
}
```

**Interpretaci√≥n de Negocio:**

- De cada 100 clientes que se van, detectas 83 ‚úÖ
- De cada 100 alertas que generas, 72 son correctas ‚úÖ
- Solo pierdes 17 de cada 100 clientes en riesgo üéØ

---

## üîç **Resumen Visual del Flujo**

```
1. Entrenar modelo ‚Üí model.fit(X_train, y_train)
2. Hacer predicciones ‚Üí y_pred = model.predict(X_test)
3. Obtener probabilidades ‚Üí y_pred_proba = model.predict_proba(X_test)[:, 1]
4. Calcular m√©tricas:
   ‚îú‚îÄ accuracy_score(y_test, y_pred)
   ‚îú‚îÄ precision_score(y_test, y_pred)
   ‚îú‚îÄ recall_score(y_test, y_pred)
   ‚îú‚îÄ f1_score(y_test, y_pred)
   ‚îî‚îÄ roc_auc_score(y_test, y_pred_proba)
5. Visualizar:
   ‚îú‚îÄ confusion_matrix()
   ‚îú‚îÄ roc_curve()
   ‚îî‚îÄ precision_recall_curve()
```

¬øTe gustar√≠a que profundice en alguna m√©trica espec√≠fica o que te muestre c√≥mo interpretar las visualizaciones en m√°s detalle?
