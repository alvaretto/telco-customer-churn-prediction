---
title: "An√°lisis Detallado: Optimizaci√≥n de Hiperpar√°metros"
author: "Bootcamp VirtIA"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
  pdf_document:
    latex_engine: xelatex
    keep_tex: false
    toc: true
    toc_depth: 3
---

# üìä An√°lisis Detallado: Optimizaci√≥n de Hiperpar√°metros

He analizado la celda "8. Optimizaci√≥n de Hiperpar√°metros" del notebook. Aqu√≠ est√° el an√°lisis completo:

## 1. **Razones del Tiempo de Ejecuci√≥n Prolongado**

### üîç Configuraci√≥n Actual

```python
# path=Telco_Customer_Churn.ipynb mode=EXCERPT
random_search = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_distributions,
    n_iter=50,
    cv=5,
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)
```

**Detalles t√©cnicos:**

- **Algoritmo:** `RandomizedSearchCV` (b√∫squeda aleatoria, m√°s eficiente que GridSearchCV)
- **Modelo:** `RandomForestClassifier` (computacionalmente costoso)
- **Iteraciones:** 50 combinaciones aleatorias
- **Validaci√≥n cruzada:** 5 folds (StratifiedKFold impl√≠cito)
- **Dataset:** 8,278 filas √ó 39 columnas (despu√©s de SMOTE)
- **Paralelizaci√≥n:** `n_jobs=-1` ‚úÖ (ya activada)

### üìê Espacio de B√∫squeda

```python
# path=Telco_Customer_Churn.ipynb mode=EXCERPT
param_distributions = {
    'n_estimators': [100, 200, 300, 500],      # 4 opciones
    'max_depth': [10, 20, 30, None],           # 4 opciones
    'min_samples_split': [2, 5, 10],           # 3 opciones
    'min_samples_leaf': [1, 2, 4],             # 3 opciones
    'max_features': ['sqrt', 'log2'],          # 2 opciones
    'bootstrap': [True, False]                 # 2 opciones
}
```

**Combinaciones posibles:** 4 √ó 4 √ó 3 √ó 3 √ó 2 √ó 2 = **576 combinaciones**  
**Combinaciones evaluadas:** 50 (aleatorias)  
**Total de entrenamientos:** 50 √ó 5 = **250 fits** ‚úÖ (confirmado en output)

---

## 2. **C√°lculo Estimado del Tiempo**

### ‚è±Ô∏è F√≥rmula de Tiempo

```
Tiempo Total = (n_iter) √ó (cv_folds) √ó (tiempo_por_modelo)
```

### üìä Estimaci√≥n Detallada

| Factor | Valor | Impacto |
|--------|-------|---------|
| **Iteraciones** | 50 | Alto |
| **Folds CV** | 5 | Alto |
| **Total fits** | 250 | Muy Alto |
| **Muestras** | 8,278 | Alto (duplicado por SMOTE) |
| **Features** | 39 | Moderado |
| **Max n_estimators** | 500 | Muy Alto |

**Complejidad de Random Forest:**
```
O(n_estimators √ó n_samples √ó n_features √ó log(n_samples))
O(500 √ó 8,278 √ó 39 √ó log(8,278))
O(500 √ó 8,278 √ó 39 √ó 13.02) ‚âà 2,100 millones de operaciones por modelo
```

**Tiempo estimado por modelo:**

- Con `n_estimators=100`: ~2-3 segundos
- Con `n_estimators=500`: ~10-15 segundos
- **Promedio:** ~5 segundos por fit

**Tiempo total estimado:**
```
250 fits √ó 5 segundos = 1,250 segundos ‚âà 20-21 minutos
```

### üî• Por qu√© es tan costoso:

1. **Random Forest con 500 √°rboles** es extremadamente pesado
2. **Dataset balanceado con SMOTE** casi duplic√≥ el tama√±o (5,634 ‚Üí 8,278 filas)
3. **5 folds de CV** multiplica el trabajo por 5
4. **50 iteraciones** es un n√∫mero alto para b√∫squeda aleatoria
5. **39 features** despu√©s del preprocesamiento aumenta la complejidad

---

## 3. **Recomendaciones para Reducir el Tiempo** üöÄ

### ‚úÖ **Opci√≥n 1: Reducci√≥n Moderada (Recomendada)**

**Cambios sugeridos:**
```python
random_search = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_distributions_reduced,
    n_iter=20,        # ‚¨áÔ∏è Reducir de 50 a 20 (-60%)
    cv=3,             # ‚¨áÔ∏è Reducir de 5 a 3 (-40%)
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)

# Espacio de b√∫squeda reducido
param_distributions_reduced = {
    'n_estimators': [100, 200, 300],        # ‚¨áÔ∏è Eliminar 500
    'max_depth': [10, 20, None],            # ‚¨áÔ∏è Eliminar 30
    'min_samples_split': [2, 5],            # ‚¨áÔ∏è Eliminar 10
    'min_samples_leaf': [1, 2],             # ‚¨áÔ∏è Eliminar 4
    'max_features': ['sqrt', 'log2'],       # ‚úÖ Mantener
    'bootstrap': [True, False]              # ‚úÖ Mantener
}
```

**Resultado:**

- **Fits totales:** 20 √ó 3 = **60 fits** (vs 250 original)
- **Tiempo estimado:** 60 √ó 3 seg = **180 segundos ‚âà 3 minutos**
- **Reducci√≥n:** **85% m√°s r√°pido** (de ~20 min a ~3 min)
- **P√©rdida de precisi√≥n:** M√≠nima (~0.5-1% en ROC-AUC)

---

### ‚úÖ **Opci√≥n 2: Reducci√≥n Agresiva (M√°s R√°pida)**

```python
# Muestreo estratificado del dataset
from sklearn.model_selection import train_test_split

X_train_sample, _, y_train_sample, _ = train_test_split(
    X_train_balanced, 
    y_train_balanced,
    train_size=0.5,  # Usar solo 50% de los datos
    stratify=y_train_balanced,
    random_state=RANDOM_STATE
)

random_search = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_distributions_reduced,
    n_iter=15,        # ‚¨áÔ∏è Reducir a 15
    cv=3,             # ‚¨áÔ∏è 3 folds
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)

random_search.fit(X_train_sample, y_train_sample)  # ‚¨áÔ∏è Usar muestra
```

**Resultado:**

- **Dataset:** 4,139 muestras (50% del balanceado)
- **Fits totales:** 15 √ó 3 = **45 fits**
- **Tiempo estimado:** 45 √ó 1.5 seg = **67 segundos ‚âà 1 minuto**
- **Reducci√≥n:** **95% m√°s r√°pido**
- **P√©rdida de precisi√≥n:** Moderada (~1-2% en ROC-AUC)

---

### ‚úÖ **Opci√≥n 3: Valores Acotados por Conocimiento Previo**

Bas√°ndome en los resultados obtenidos, puedo acotar a√∫n m√°s:

```python
# path=Telco_Customer_Churn.ipynb mode=EXCERPT
# Mejores hiperpar√°metros encontrados:
{'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1,
 'max_features': 'log2', 'max_depth': 20, 'bootstrap': False}
```

**Espacio acotado basado en resultados:**
```python
param_distributions_focused = {
    'n_estimators': [200, 300, 400],        # Rango cercano a 500
    'max_depth': [15, 20, 25],              # Rango cercano a 20
    'min_samples_split': [2, 3],            # Valores bajos
    'min_samples_leaf': [1, 2],             # Valores bajos
    'max_features': ['log2'],               # Solo el mejor
    'bootstrap': [False, True]              # Priorizar False
}
```

**Resultado:**

- **Combinaciones posibles:** 3 √ó 3 √ó 2 √ó 2 √ó 1 √ó 2 = 72
- **Con n_iter=15, cv=3:** 15 √ó 3 = **45 fits**
- **Tiempo:** ~**2 minutos**
- **Precisi√≥n:** Muy similar al original

---

## 4. **Alternativas M√°s R√°pidas** ‚ö°

### üéØ **Estrategia H√≠brida (Mejor Opci√≥n)**

```python
# FASE 1: B√∫squeda r√°pida con espacio amplio
param_distributions_phase1 = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True, False]
}

random_search_phase1 = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_distributions_phase1,
    n_iter=10,  # Solo 10 iteraciones
    cv=3,
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1
)

random_search_phase1.fit(X_train_balanced, y_train_balanced)
best_params_phase1 = random_search_phase1.best_params_

# FASE 2: Refinamiento alrededor de los mejores par√°metros
param_distributions_phase2 = {
    'n_estimators': [best_params_phase1['n_estimators'], 
                     best_params_phase1['n_estimators'] + 100,
                     best_params_phase1['n_estimators'] + 200],
    'max_depth': [best_params_phase1['max_depth']],  # Fijo
    'min_samples_split': [best_params_phase1['min_samples_split']],  # Fijo
    'min_samples_leaf': [best_params_phase1['min_samples_leaf']],  # Fijo
    'max_features': [best_params_phase1['max_features']],  # Fijo
    'bootstrap': [best_params_phase1['bootstrap'], 
                  not best_params_phase1['bootstrap']]
}

random_search_phase2 = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_distributions_phase2,
    n_iter=6,
    cv=5,  # M√°s riguroso en fase 2
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1
)

random_search_phase2.fit(X_train_balanced, y_train_balanced)
```

**Ventajas:**

- **Tiempo total:** ~5-7 minutos (vs 20 minutos)
- **Precisi√≥n:** Similar o mejor que b√∫squeda √∫nica
- **Exploraci√≥n + Explotaci√≥n:** Mejor balance

---

### üìä **Comparaci√≥n de Opciones**

| Opci√≥n | Tiempo | Fits | Precisi√≥n | Recomendaci√≥n |
|--------|--------|------|-----------|---------------|
| **Original** | ~20 min | 250 | 100% | ‚ùå Muy lento |
| **Moderada** | ~3 min | 60 | ~99% | ‚úÖ **Mejor balance** |
| **Agresiva** | ~1 min | 45 | ~98% | ‚ö†Ô∏è Solo para pruebas r√°pidas |
| **Acotada** | ~2 min | 45 | ~99.5% | ‚úÖ Si conoces buenos rangos |
| **H√≠brida** | ~5-7 min | 60 | ~99.5% | ‚úÖ Mejor exploraci√≥n |

---

## üéØ **Recomendaci√≥n Final para Google Colab**

Para ejecutar en Google Colab de forma eficiente, usa la **Opci√≥n 1 (Moderada)**:

```python
# Configuraci√≥n optimizada para Colab
param_distributions_colab = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True, False]
}

random_search_colab = RandomizedSearchCV(
    estimator=rf_base,
    param_distributions=param_distributions_colab,
    n_iter=20,
    cv=3,
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=2  # M√°s informaci√≥n durante ejecuci√≥n
)

random_search_colab.fit(X_train_balanced, y_train_balanced)
```

**Beneficios:**

- ‚è±Ô∏è **Tiempo:** ~3 minutos (85% m√°s r√°pido)
- üéØ **Precisi√≥n:** P√©rdida m√≠nima (<1%)
- üí∞ **Recursos:** Compatible con Colab gratuito
- üîÑ **Iteraciones:** Permite m√∫ltiples experimentos
